[
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html",
    "href": "Projects/Project_02/scripts/Analysis.html",
    "title": "Contex",
    "section": "",
    "text": "Una empresa financiera tiene un nuevo producto dirigido a sus clientes con diferentes límites de crédito, por ello, es necesario dividir a los clientes en grupos más pequeños.\nDesafortunadamente, los directivos no tienen idea de cuál es la cantidad óptima en la que se puede segregar a los clientes y cómo dirigir a nuevos clientes a esos grupos.\nEs por eso que en este punto se te ha contratado. Para poder ejecutar tu trabajo te han compartido una base de datos que contiene las características que, según sus informes, son los más importantes para la aplicación de este programa. Consiste en seis puntos: - Salario mensual: indica el ingreso mensual del cliente en pesos mexicanos. - Crédito tipo 1: es un producto financiero que el cliente ha obtenido de la institución financiera; 1 significa que tiene este producto y 0 que no cuenta con él. - Crédito tipo 2: es similar a las características del anterior; 1 significa que cuenta con el producto y 0 que no. - Límite de TC: indica el límite de crédito de la tarjeta del cliente. - Años siendo cliente: corresponde al tiempo que el cliente ha tenido una relación comercial con la institución financiera. - Previamente se ofreció el producto: es la versión previa del producto e indica si el cliente se mostró interesado en obtenerlo (1) o no (0).\nEl archivo puede ser descargado aquí: https://drive.google.com/file/d/1wcjPCZPTMuDlyWcE1Do4-yqU5AglW_p9/view?usp=sharing"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#import-libraries",
    "href": "Projects/Project_02/scripts/Analysis.html#import-libraries",
    "title": "Contex",
    "section": "0.1 Import Libraries",
    "text": "0.1 Import Libraries\n\nfrom pandas import read_csv, DataFrame\n\n\nfrom numpy import where, arange, cumsum\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import (KMeans,\n                             AgglomerativeClustering,\n                             DBSCAN,\n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (silhouette_score,\n                             calinski_harabasz_score,\n                             davies_bouldin_score,\n)\n\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nfrom seaborn import (boxplot,\n                     distplot,\n                     set_style,\n                     displot,\n                     countplot,\n                     scatterplot,\n                     barplot,\n                     lmplot,\n                     heatmap,\n)\n\n\nimport scipy.cluster.hierarchy as shc\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n!pip install kneed\nfrom kneed import KneeLocator\n\nCollecting kneed\n  Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\nRequirement already satisfied: numpy&gt;=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.11.4)\nInstalling collected packages: kneed\nSuccessfully installed kneed-0.8.5"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#load-data",
    "href": "Projects/Project_02/scripts/Analysis.html#load-data",
    "title": "Contex",
    "section": "0.2 Load Data",
    "text": "0.2 Load Data\n\ndf = read_csv('/content/drive/MyDrive/Colab Notebooks/proyectos/costumers.csv')\ndisplay(df.head())\n\n\n  \n    \n\n\n\n\n\n\nSalario\nCredito_tipo_1\nCredito_tipo_2\nLimite_TC\nTiempo_cliente\nProducto_ofrecido\n\n\n\n\n0\n22572.91\n1\n0\n30136.74\n0.0\n1\n\n\n1\n10609.64\n1\n1\n53623.94\n0.0\n1\n\n\n2\n10079.48\n0\n1\n18135.44\n0.0\n0\n\n\n3\n13871.35\n1\n0\n30831.75\n0.0\n1\n\n\n4\n6541.46\n0\n1\n20626.23\n0.0\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nDefinimos nuestro DataFrame como df y observamos las primeras cinco muestras"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#eda",
    "href": "Projects/Project_02/scripts/Analysis.html#eda",
    "title": "Contex",
    "section": "0.3 EDA",
    "text": "0.3 EDA\n\ndf.describe(include='all').T\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSalario\n500.0\n12089.09738\n5123.700116\n-2911.65\n8534.350\n11438.600\n15171.4675\n30041.40\n\n\nCredito_tipo_1\n500.0\n0.61000\n0.488238\n0.00\n0.000\n1.000\n1.0000\n1.00\n\n\nCredito_tipo_2\n500.0\n0.45800\n0.498732\n0.00\n0.000\n0.000\n1.0000\n1.00\n\n\nLimite_TC\n500.0\n44784.55496\n22150.923996\n-22207.95\n30096.285\n47991.515\n59108.1750\n118636.82\n\n\nTiempo_cliente\n500.0\n4.32700\n4.784776\n0.00\n0.000\n0.000\n8.4000\n19.30\n\n\nProducto_ofrecido\n500.0\n0.60200\n0.489976\n0.00\n0.000\n1.000\n1.0000\n1.00\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nObservemos las estadisticas descritivas de nuestro dataset, podemos observar que las columnas Salario y Limite_TC sus valores minimos son negativos, lo cual no tiene sentido.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Salario            500 non-null    float64\n 1   Credito_tipo_1     500 non-null    int64  \n 2   Credito_tipo_2     500 non-null    int64  \n 3   Limite_TC          500 non-null    float64\n 4   Tiempo_cliente     500 non-null    float64\n 5   Producto_ofrecido  500 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 23.6 KB\n\n\nAca mostramos la información general de nuestro dataset, desde el total de muestras hasta el tipo de dato.\n\ndf.isnull().sum()\n\nSalario              0\nCredito_tipo_1       0\nCredito_tipo_2       0\nLimite_TC            0\nTiempo_cliente       0\nProducto_ofrecido    0\ndtype: int64\n\n\nMostramos la completitud de nuestro dataset, en este caso no contiene valores faltantes.\n\n0.3.1 Comprobamos valores atípicos\n\nfig = px.box(df, y=\"Salario\", points=\"all\", width=700, height=550)\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nfig = px.box(df, y=\"Limite_TC\", points=\"all\", width=700, height=550)\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\nMediante las graficas de cajas se pueden observar numerosos valores atípicos en ambas columnas. Tambien se observan valores negativos en las columnas que no tienen sentido, por ejemplo un salario negativo\n\narg_min_sal = df.iloc[df['Salario'].argmin()]# Localizamos y mostramos los valores mínimos\narg_min_limt = df.iloc[df['Limite_TC'].argmin()]\n\nprint(f'Los montos mínimos de Salario son \\n{arg_min_sal}.\\n==================================\\nY del limite de credito \\n{arg_min_limt}.')\n\nLos montos mínimos de Salario son \nSalario              -2911.65\nCredito_tipo_1           1.00\nCredito_tipo_2           1.00\nLimite_TC            86358.80\nTiempo_cliente           0.00\nProducto_ofrecido        1.00\nName: 472, dtype: float64.\n==================================\nY del limite de credito \nSalario              29868.76\nCredito_tipo_1           0.00\nCredito_tipo_2           0.00\nLimite_TC           -22207.95\nTiempo_cliente           0.00\nProducto_ofrecido        1.00\nName: 478, dtype: float64.\n\n\nAl igual que la columna de limite de credito, cuenta con un limite negativo\n\n# Definir límites inferior y superior\nqs = df.quantile([0.25,0.5,0.75]).values\nq1 = qs[0][0]\nq2 = qs[1][0]\nq3 = qs[2][0]\n\niqr = q3-q1\niqr_lim_inf = q1 - 1.285*iqr# Modifique el limite inferior de manera que me diera el monto positivo mas cercano a 1\n\nDefinimos variables con los rangos intercuartilicos, y estructuramos los limites inferiores, que son los que usaremos\n\n# Y realizar el recorte en este caso a salario y graficamos de nuevo\ndf['Salario'] = where(df['Salario']&lt;iqr_lim_inf,\n                                 iqr_lim_inf, df['Salario'])\n\nfig = px.box(df, y='Salario', points='all', width = 700, height=500)\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Y realizar el recorte a la columna de limite de credito, y ahora se ve así\ndf['Limite_TC'] = where(df['Limite_TC']&lt;iqr_lim_inf,\n                                 iqr_lim_inf, df['Limite_TC'])\n\nfig = px.box(df, y='Limite_TC', points='all', width = 700, height=500)\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\narg_min_sal = df.iloc[df['Salario'].argmin()]\narg_min_limt = df.iloc[df['Limite_TC'].argmin()]\n\nprint(f'Los montos mas bajos de Salario son \\n{arg_min_sal}.\\n==================================\\nY del limite de credito \\n{arg_min_limt}.')\n\nLos montos mas bajos de Salario son \nSalario                  5.654013\nCredito_tipo_1           1.000000\nCredito_tipo_2           1.000000\nLimite_TC            86358.800000\nTiempo_cliente           0.000000\nProducto_ofrecido        1.000000\nName: 472, dtype: float64.\n==================================\nY del limite de credito \nSalario              24295.710000\nCredito_tipo_1           1.000000\nCredito_tipo_2           0.000000\nLimite_TC                5.654013\nTiempo_cliente           0.000000\nProducto_ofrecido        0.000000\nName: 47, dtype: float64.\n\n\nAhora las columnas tienen mas sentido, con valores positivos."
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#univariate-analysis",
    "href": "Projects/Project_02/scripts/Analysis.html#univariate-analysis",
    "title": "Contex",
    "section": "0.4 Univariate analysis",
    "text": "0.4 Univariate analysis\n\ndf.columns\n\nIndex(['Salario', 'Credito_tipo_1', 'Credito_tipo_2', 'Limite_TC',\n       'Tiempo_cliente', 'Producto_ofrecido'],\n      dtype='object')\n\n\n\n0.4.1 Salario colum\n\nplt.figure(figsize=(8,6))\nset_style('darkgrid')\n\ndisplot(df, x=\"Salario\", kde=True)\nplt.title(\"Distribución de Salario\\n========================================\", fontsize=20, color=\"blue\")\nplt.xlabel(\"Rango de Salario\", fontsize=15)\nplt.ylabel(\"Densidad\", fontsize=15)\n\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLa mayor parte de los salarios se encuentran entre los 8.000 y 15.000\n\n\n0.4.2 Credito tipo 1\n\ndf['Credito_tipo_1'].unique()\n\narray([1, 0])\n\n\n\nplt.figure(figsize=(6,4))\ndf['Credito_tipo_1'].value_counts().plot(kind='bar', color=['blue', 'red'])\nplt.xlabel('Aprobado                                      No aprobado')\nplt.ylabel('Cantidad')\nplt.title('Credito tipo 1')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,6))\ndf['Credito_tipo_1'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Creditos tipo 1')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nPodemos observar que tiene mayor tasa de aprobación credito tipo 1.\n\n\n0.4.3 Credito tipo 2\n\ndf['Credito_tipo_2'].nunique()\n\n2\n\n\n\nplt.figure(figsize=(6,4))\ndf['Credito_tipo_2'].value_counts().plot(kind='bar', color=['blue', 'red'])\nplt.xlabel('  No Aprobado                              Aprobado')\nplt.ylabel('Cantidad')\nplt.title('Credito tipo 2')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,6))\ndf['Credito_tipo_2'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Creditos tipo 2')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nA diferencia del credito tipo 1, este credito tipo 2 su tasa de aceptación es un poco mas baja.\n\n\n0.4.4 Limite_TC colum\n\n\nplt.figure(figsize=(8,6))\nset_style('darkgrid')\n\ndisplot(df, x=\"Limite_TC\", kde=True)\nplt.title(\"Distribución de Limite de Credito\\n=====================================\", fontsize=20, color=\"blue\")\nplt.xlabel(\"Rango de Limite de Credito\", fontsize=15)\nplt.ylabel(\"Densidad\", fontsize=15)\n\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLa mayor parte del limite de credito estan entre los 45.000 y 60.000. Y hay una cantidad considerable las cuales no cuentan con credito.\n\n\n0.4.5 Tiempo cliente colum\n\ndf['Tiempo_cliente'].value_counts()\n\nTiempo_cliente\n0.0     254\n7.0       8\n7.2       7\n7.3       7\n8.5       6\n       ... \n15.0      1\n13.1      1\n7.8       1\n10.6      1\n15.3      1\nName: count, Length: 88, dtype: int64\n\n\n\nplt.figure(figsize=(10,6))\nset_style('darkgrid')\n\ndisplot(df, x=\"Tiempo_cliente\", kde=True)\nplt.title(\"Distribución de tiempo del cliente\\n=============================================\", fontsize=20, color=\"blue\")\nplt.xlabel(\"Rango de tiempo\", fontsize=15)\nplt.ylabel(\"Densidad\", fontsize=15)\n\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLa distribución no esta nada pareja y fluctua de manera particual, aqui nos muestra que la gran mayoria cuentan con 0-1 año siendo cliente, luego entre los 7-11 años su segundo peack.\n\n\n0.4.6 Producto ofrecido\n\ndf['Producto_ofrecido'].value_counts()\n\nProducto_ofrecido\n1    301\n0    199\nName: count, dtype: int64\n\n\n\nplt.figure(figsize=(6,4))\ndf['Producto_ofrecido'].value_counts().plot(kind='bar', color=['blue', 'red'])\nplt.xlabel('  Aprobado                              No aprobado')\nplt.ylabel('Cantidad')\nplt.title('Producto ofrecido')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\nEn su mayoria se cuenta con clientes con el producto ofrecido."
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#analisis-bivariado",
    "href": "Projects/Project_02/scripts/Analysis.html#analisis-bivariado",
    "title": "Contex",
    "section": "0.5 Analisis bivariado",
    "text": "0.5 Analisis bivariado\n\ndf.columns\n\nIndex(['Salario', 'Credito_tipo_1', 'Credito_tipo_2', 'Limite_TC',\n       'Tiempo_cliente', 'Producto_ofrecido'],\n      dtype='object')\n\n\n\ncorr = df.corr()\nheatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Mapa de Calor de Correlación')\nplt.show()\n\n\n\n\n\n\n\n\nAcá podemos observar un mapa de calor con la correlación de nuestras features, logramos ver la mayor relacion que seria Limite_TC con Tiempo_cliente, nos indica que mientras mas tiempo se este en el banco en este caso, incrementa el limite crediticio; seguido con Credito_tipo_1 y por ultimo con Producto_ofrecido, nos indica que mientras mas tiempo siendo cliente, la tasa de aceptación de producto incrementa.\n\nplt.figure(figsize=(10, 6))\nscatterplot(data=df, x='Tiempo_cliente', y='Limite_TC', marker='o')\n\n# Añadir títulos y etiquetas\nplt.title('Relación entre Limite credito y Periodo de Tiempo del Cliente')\nplt.xlabel('Periodo de Tiempo del Cliente (años)')\nplt.ylabel('Limite credito')\nplt.grid(True)\n\n\n\n\n\n\n\n\nhay una concentración significativa de puntos a la izquierda del eje X (en el periodo de tiempo cercano a 0), indicando que hay muchos clientes con un periodo de tiempo muy corto.\nSe observa una tendencia ligera de incremento en el límite de crédito a medida que aumenta el periodo de tiempo del cliente. Los clientes que han estado más tiempo tienden a tener límites de crédito más altos.\n\nplt.figure(figsize=(8,6))\nset_style('darkgrid')\nscatterplot(data=df, x=\"Salario\", y= \"Limite_TC\", hue=\"Producto_ofrecido\", s=65, alpha= 0.8)\nplt.title(\"Salario vs Limite de Credito\\n===================================\", fontsize=20, color=\"blue\")\nplt.xlabel(\"Salario\", fontsize=15)\nplt.ylabel(\"Limite de credito\", fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\nAplicamos una grafica de dispersión con los features Salario y Limite de credito, con el parametro de producto ofrecido para ver su comportamiento. Podemos ver que los que las personas con mayor limite de credito tienden a tener el producto ofrecido, mientras que las personas con menos limite tienden a no poseerlo.\n\ndf.groupby(['Producto_ofrecido', 'Credito_tipo_1']).size().unstack().plot(kind='bar', stacked=True)\nplt.xlabel('Producto ofrecido')\nplt.ylabel('Frecuencia')\nplt.title('Gráfico de Barras Apiladas')\nplt.show()\n\n\n\n\n\n\n\n\nPodemos ver que los que cuentan con el Credito tipo 1, en su mayoria cuentan con el producto ofrecido, y en su minoria, los que no cuentan con el Credito tipo 1"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#data-preprocessing",
    "href": "Projects/Project_02/scripts/Analysis.html#data-preprocessing",
    "title": "Contex",
    "section": "0.6 Data preprocessing",
    "text": "0.6 Data preprocessing\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nSalario\nCredito_tipo_1\nCredito_tipo_2\nLimite_TC\nTiempo_cliente\nProducto_ofrecido\n\n\n\n\n0\n22572.91\n1\n0\n30136.74\n0.0\n1\n\n\n1\n10609.64\n1\n1\n53623.94\n0.0\n1\n\n\n2\n10079.48\n0\n1\n18135.44\n0.0\n0\n\n\n3\n13871.35\n1\n0\n30831.75\n0.0\n1\n\n\n4\n6541.46\n0\n1\n20626.23\n0.0\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.drop(columns=['Credito_tipo_1', 'Credito_tipo_2'], inplace=True)\n\n\n0.6.1 Scaling\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n\ndefinimos el metodo de escalamiento que usaremos, en este caso Z-score, como tenemos valores atípicos que creo que son importantes, nos sera mas utíl por su robustez y eficiencia ante valores atípicos\n\n\n0.6.2 Elbow method\n\ninertia = [] # Lista vacía\nfor i in range(1, 11):# Bucle for en rango de 1 a 10 para instanciar en cada valor, luego guardar resultados en lista vacía\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(df_scaled)\n    inertia.append(kmeans.inertia_)\n\n# Graficar el codo\nplt.plot(range(1, 11), inertia, marker='o')\nplt.title('Método del Codo')\nplt.xlabel('Número de Clusters')\nplt.ylabel('Inercia')\nplt.show()\n\n\n\n\n\n\n\n\nAplicaremos metodo del codo para buscar el valor optimo de K mediante la inercía, se toma el valor en el cual la grafica se inclina mas, en este caso usaremos K=4\n\n\n0.6.3 Knee method\n\nkneedle = KneeLocator(range(1,11), inertia, curve='convex', direction='decreasing')\nelbow_point =  kneedle.elbow\nprint(elbow_point)\n\n4\n\n\nComo segunda opción usaremos el metodo de ‘rodilla’ que igualmente hace uso de la inercia y así arrojar el valor optimo de K\n\n\n0.6.4 PCA\n\n#n_components = 2\npca = PCA(n_components=2)\nscaled_pca = pca.fit_transform(df)\nprint(pca.explained_variance_ratio_)\n\n[0.95320342 0.04679656]\n\n\nPara una mejor visualización de los Cluster usaremos una reducción de dimensiones con PCA, se lo aplicaremos en nuestro dataset escalado"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#kmeans-cluster",
    "href": "Projects/Project_02/scripts/Analysis.html#kmeans-cluster",
    "title": "Contex",
    "section": "0.7 KMeans cluster",
    "text": "0.7 KMeans cluster\n\n#K = 4\nkmeans = KMeans(n_clusters=4,\n                random_state=40).fit(scaled_pca)\n\ndf_pca = DataFrame(scaled_pca, columns=['PCA1', 'PCA2'])\ndf['Cluster'] = kmeans.labels_\ndf_pca['Cluster'] = kmeans.labels_\n\nInstanciamos y ajustamos nuestro metodo de clustering, en este caso KMeans, y usaremos el valor optimo K=4 que nos arrojo los metodos anteriores, luego creamos un dataframe que contendra los componentes principales, y visualizarlo de manera mas facíl, agregando una columna de las agrupaciones(cluster)\n\nplt.figure(figsize=(8, 6))\nscatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_pca, palette='viridis')\nplt.title('Visualización de Clusters')\nplt.show()\n\n\n\n\n\n\n\n\nCreamos un dataframe que contenga los dos componentes principales y la columna de Cluster creada con KMeans, para observar como se comporta y sus agrupaciones\n\nplt.figure(figsize=(8, 6))\nscatterplot(x='Salario', y='Limite_TC', hue='Cluster', data=df, palette='viridis')\nplt.title('Visualización de Clusters')\nplt.show()\n\n\n\n\n\n\n\n\nUsamos la misma grafica anterior, con la diferencia que la aplicamos al dataframe original escalado, podemos ver que cambian las agrupaciones"
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#agglomerative-clustering",
    "href": "Projects/Project_02/scripts/Analysis.html#agglomerative-clustering",
    "title": "Contex",
    "section": "0.8 Agglomerative Clustering",
    "text": "0.8 Agglomerative Clustering\n\n# Aplicar Agglomerative Clustering\nagglomerative_clustering = AgglomerativeClustering(n_clusters=4,\n                                                   affinity='euclidean',\n                                                   linkage='ward')\n\ndf['Cluster'] = agglomerative_clustering.labels_\nscaled_pca['Cluster'] = agglomerative_clustering.labels_\n\nPara tener una idea diferente usaremos otra tecnica de clustering, en este caso usaremos Agglomerative clustering\n\nplt.figure(figsize=(8, 6))\nscatterplot(data=df_pca, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=50)\nplt.title('Visualización de Clusters con Agglomerative Clustering')\nplt.xlabel('PCA1')\nplt.ylabel('PCA2')\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\n\n\n\n\nAplicamos una grafica de dispersión, en este caso al dataframe con los componentes principales.\n\nplt.figure(figsize=(8, 6))\nscatterplot(data=df, x='Salario', y='Limite_TC', hue='Cluster', palette='viridis', s=50)\nplt.title('Visualización de Clusters con Agglomerative Clustering')\nplt.xlabel('Salario')\nplt.ylabel('Limite_TC')\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\n\n\n\n\nY al igual que las graficas anteriores, acá se la aplcamos al dataframe original con las muestras escaladas."
  },
  {
    "objectID": "Projects/Project_02/scripts/Analysis.html#aplicación-de-metricas-de-desempeño",
    "href": "Projects/Project_02/scripts/Analysis.html#aplicación-de-metricas-de-desempeño",
    "title": "Contex",
    "section": "0.9 Aplicación de metricas de desempeño",
    "text": "0.9 Aplicación de metricas de desempeño\n\nsilhouette_avg = silhouette_score(df, df['Cluster'])\ndb_score = davies_bouldin_score(df, df['Cluster'])\nch_score = calinski_harabasz_score(df, df['Cluster'])\nprint(f'Índice de Silueta: {silhouette_avg}')\nprint(f'El coeficiente del índice Calinski-Harabasz es de {ch_score}.')\nprint(f'El coeficiente del índice Davies-Bouldin es de {db_score}.')\n\nÍndice de Silueta: 0.45615259890267384\nEl coeficiente del índice Calinski-Harabasz es de 943.0221360184264.\nEl coeficiente del índice Davies-Bouldin es de 0.7033885733580293.\n\n\n\nUn valor de silueta más cercano a 1 indica un mejor rendimiento\nCon la metrica davies_bouldin un valor más alto es mejor\nLa metrrica de calinski nos dice que un valor más bajo es mejor"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#importing-libraries",
    "href": "Projects/Project_01/scripts/analysis.html#importing-libraries",
    "title": "1 Context",
    "section": "1.1 Importing Libraries",
    "text": "1.1 Importing Libraries\n\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (Dropout,\n                                     Dense,\n                                     SimpleRNN,\n                                     LSTM,\n                                     Conv2D,\n                                     Embedding,\n                                     MaxPooling2D,\n                                     Flatten,\n)\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n\nimport numpy as np\nfrom pandas import (read_csv,\n                    DataFrame,\n)\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom seaborn import countplot, heatmap\n\n\nfrom PIL import Image\n\n\nimport requests\n\n\nfrom bs4 import BeautifulSoup\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#load-dataset",
    "href": "Projects/Project_01/scripts/analysis.html#load-dataset",
    "title": "1 Context",
    "section": "1.2 Load Dataset",
    "text": "1.2 Load Dataset\n\ndf = read_csv('/content/drive/MyDrive/Colab Notebooks/proyectos/WELFake_Dataset.csv')\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nlabel\n\n\n\n\n0\n0\nLAW ENFORCEMENT ON HIGH ALERT Following Threat...\nNo comment is expected from Barack Obama Membe...\n1\n\n\n1\n1\nNaN\nDid they post their votes for Hillary already?\n1\n\n\n2\n2\nUNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...\nNow, most of the demonstrators gathered last ...\n1\n\n\n3\n3\nBobby Jindal, raised Hindu, uses story of Chri...\nA dozen politically active pastors came here f...\n0\n\n\n4\n4\nSATAN 2: Russia unvelis an image of its terrif...\nThe RS-28 Sarmat missile, dubbed Satan 2, will...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nCargamos el dataset que usaremos para el entrenamiento de nuetro modelo, y lo definimos como df\n##EDA\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 72134 entries, 0 to 72133\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  72134 non-null  int64 \n 1   title       71576 non-null  object\n 2   text        72095 non-null  object\n 3   label       72134 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 2.2+ MB\n\n\nMostramos la información general del dataset; Se muestran los tipos de datos de cada columna, el total de muestras por columna y la existencia de valores nulos. Podemos ver la primera columna la cual no nos serviria para nada, ya que seria como un identificador numerico de cada uno de los valores.\n\ndf.duplicated().sum()\n\n0\n\n\nAcá podremos observar la sumatoria de todos los valores duplicados del dataset, de no existir algun valor duplicado nos imprime un 0\n\ndf.isnull().sum()\n\nUnnamed: 0      0\ntitle         558\ntext           39\nlabel           0\ndtype: int64\n\n\n\nprint(df.isnull().sum() / len(df)*100)\n\nUnnamed: 0    0.000000\ntitle         0.773560\ntext          0.054066\nlabel         0.000000\ndtype: float64\n\n\nMostramos los valores faltantes de las columnas a trabajar, podemos observar que hay datos faltantes, pero es tan minimo que serian irrelevantes para el dataset\n\n\n\nEn términos generales, se suelen considerar los siguientes grados de impacto, dependiendo del porcentaje de valores faltantes (dumb rules):\n\nMenos de 1%: Trivial (no relevante)\n1-5%: Manejable\n5-15%: Manejable mediante métodos sofisticados\nMás de 15%: Crítico, con impacto severo en cualquier tipo de interpretación\n\n\n\n1.3 Data Preprocessing\n\ndf.drop(columns=['Unnamed: 0'], inplace=True)\n\nAcá borramos dicha columna que no usaremos\n\ndf.dropna(inplace=True)\n\n\ndf = df.reset_index(drop=True)\n\nEliminamos los valores faltantes, y reestablecemos los indices del dataframe\n\n\n1.4 Univariate Analysis\n\ndf['label'].value_counts().plot.pie(autopct='%.2f')\n\n\n\n\n\n\n\n\n\ndf['label'].value_counts()\n\n1    36509\n0    35028\nName: label, dtype: int64\n\n\nMediante la grafica pie o pastel, observamos que la columna label que contiene si la notica es falsa o verdadera, esta muy equilibrada bastante parejo\n\n\n1.5 Train and test data\n\nX = df['text']\ny = df['label']\n\n# Dividir los datos en conjunto de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=42)\n\nAntes de llevar acabo la tokenización, definimos nuestras variables X y y, en este caso usare solo la columna text para entrenar mi modelo, la columna title no creo que aporte al modelo, al ser un clickbait aportaria datos erroneos al modelo.\n##Tokenization\n\n# Definir el TextVectorization con salida de enteros\ntext_vectorizer = TextVectorization(max_tokens=10000,\n                                    output_mode='int', # El tipo de salida\n                                    standardize='lower', # Optimizador\n                                    output_sequence_length=500, # Longitud de salida\n                                    pad_to_max_tokens=500,\n                                    )\ntext_vectorizer.adapt(X_train)\nX_train = text_vectorizer(X_train)\nX_test = text_vectorizer(X_test)\n\nDefinimos una variable con el TextVectorization para transformar el texto y poder trabajar los datos, le aplicamos unos cuantos hiperparametros para mejorar la salida de los datos.\nAdaptamos con la variable X_train y luego vectorizamos sobreescribiendo las variables de conjunto X\n\n\n1.6 Define the Neural Network\n\n# Definir el modelo\nmodel = Sequential()\n\nmodel.add(Embedding(10000, 128, input_length=500))\n\n\nmodel.add(LSTM(128))\n\n# Agregar una capa densa final con activación sigmoid para la clasificación binaria\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 500, 128)          1280000   \n                                                                 \n lstm (LSTM)                 (None, 128)               131584    \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1411713 (5.39 MB)\nTrainable params: 1411713 (5.39 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nDefinimos nuestro modelo, en este caso utilizando la arquitectura sequential en Keras, consta de una capa de embedding(128 dim con vocabulario de 10.000) para convertir los números enteros en vectores de embeddings, seguida de una capa LSTM(128Unidades) para modelar las dependencias temporales en los datos de secuencia, y finalmente una capa densa con activación sigmoidal para la clasificación binaria.\n\n\n1.7 Compile the model\n\n# Compilamos el modelo\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nCompilamos el modelo especificando la función de pérdida y el optimizador, este caso con binary_crossentropy y adam, con la metrica de desempeño accuracy\n\n\n1.8 Train the model\n\n# Entrenamos el modelo\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=42)\n\nEpoch 1/3\n1363/1363 [==============================] - 87s 61ms/step - loss: 0.5365 - accuracy: 0.7056\nEpoch 2/3\n1363/1363 [==============================] - 39s 29ms/step - loss: 0.1237 - accuracy: 0.9578\nEpoch 3/3\n1363/1363 [==============================] - 32s 23ms/step - loss: 0.0371 - accuracy: 0.9887\n\n\nEntrenamos nuestro modelo el cual definimos como history, y lo haremos con 3 epocas en este caso. En la sección de evaluación del modelo lo explicare.\n\n\n1.9 Evaluate the model\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax1.plot(history.history['loss'])\nax1.set_title('Función de pérdida del conjunto de entrenamiento')\nax2 = fig.add_subplot(2,1,2, sharex= ax1)\nax2.plot(history.history['accuracy'])\nax2.set_title('Precisión del conjunto de entrenamiento')\n\nplt.setp(ax1.get_xticklabels(), visible=False)\n\nplt.show()\n\n\n\n\n\n\n\n\nGraficamos la función de perdida y la precisión del modelo conforme avanza de epocas en el entrenamiento. Ya en la epoca 2 se consiguio el 95% de precisión con una perdida de información del 12%, a la epoca 3 era suficiente para el entrenamiento cuidando los recursos disponibles\n\n# Evaluar el modelo en el conjunto de prueba\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Loss: {loss}, Accuracy: {accuracy}')\n\n448/448 [==============================] - 4s 8ms/step - loss: 0.0447 - accuracy: 0.9848\nLoss: 0.04466693475842476, Accuracy: 0.9848336577415466\n\n\nEvaluamos nuestro modelo con las metricas de precisión y función de perdida. Nos arroja una perdida de información de un casí 4% y una precisión del 98%\n\n\n1.10 Predict test data\n\npredicciones = model.predict(X_test)\nprint(predicciones)\n\n448/448 [==============================] - 4s 7ms/step\n[[9.9916887e-01]\n [4.8479729e-04]\n [2.8683492e-03]\n ...\n [9.9893659e-01]\n [3.7822025e-03]\n [9.9452496e-01]]\n\n\nAplicamos un predict con nuestro modelo al conjunto de prueba X_test, y lo definimos como predicciones\n\npredicciones_flat = predicciones.flatten()\npredic_si = DataFrame({'Real': y_test,\n                     'Predicción': predicciones_flat})\npredic_si.head(10)\n\n\n  \n    \n\n\n\n\n\n\nReal\nPredicción\n\n\n\n\n53925\n1\n0.999169\n\n\n6939\n0\n0.000485\n\n\n28713\n0\n0.002868\n\n\n65420\n1\n0.996083\n\n\n2286\n0\n0.001370\n\n\n34184\n1\n0.998988\n\n\n29103\n1\n0.994738\n\n\n26688\n0\n0.001819\n\n\n42956\n1\n0.958652\n\n\n65156\n1\n0.999581\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nLes mostrare una comparación con los datos reales frente a los predichos, para poder mostrarlo en un dataframe, aplane las muestras para poder trabajarlas en un dataframe\n\numbral = 0.5  # Umbral de decisión para la clasificación binaria\npredicciones_binarias = np.where(predicciones &gt;= umbral, 1, 0)\n\n# Calcular la matriz de confusión\ncm = confusion_matrix(y_test, predicciones_binarias)\n\nPara lograr graficar nuestra matriz de confusión y observar el desempeño, ambas varibles deben ser de valor 0 o 1, es decir, es un problema de clasificación binaria. Definimos una variable con un umbrar definido, de manera que los “rendondee” a 0 o 1\n\nheatmap(cm, annot= True, fmt= '.0f', cmap= 'winter')\nplt.title('Matriz de confusión')\nplt.xlabel('Valor predicho')\nplt.ylabel('Valor real')\nplt.show()\n\n\n\n\n\n\n\n\nGraficamos el desempeño del modelo con esta matriz de confusión la cual nos arroja un buen desempeño, ya que ha acertado en su gran mayoria a los True positive  y True negative\nYa con nuestro modelo listo, lo usaremos para corroborar si las siguientes noticias son falsas o verdaderas.\n#Parse a fakenew\n\n\n1.11 Load Data\n\n\n1.12 HTLM extraction\n\nurl = \"https://www.breitbart.com/politics/2016/09/10/exposed-fbi-director-james-comeys-clinton-foundation-connection/\"\n\nDefinimos una variable con la dirección de la pagina web donde proviene la noticia\n\nresponse = requests.get(url)\nif response.status_code != 200:\n    raise Exception(\"URL was not found\")\n\ntext = response.text\n\nAcá sacamos toda la información de la pagina web, en caso de ser una dirección erronea, me arrojaria una advertencia\n\n#display(text)\n\nAsí se ve la noticia antes del pre-procesamiento\n\n\n1.13 Text Processing\n\nsoup = BeautifulSoup(text, \"html.parser\")\n\n\nmain = soup.find(id=\"MainW\")\n\nparagraphs = main.findAll(\"p\")\n\ncontent = \"\"\nfor paragraph in paragraphs:\n    content = content + paragraph.get_text()\n\ntexto_lista = [content]\n\nAcá definimos una variable con la extracción del texto del HTLM, luego iteramos por parrafos y seleccionado con el id especificamente la noticia y todo almacenandolo en la variable texto lista. Para aplicarle la tokenización tuve que hacerla lista, por eso lo de la linea 9.\n\n#print(texto_lista)\n\nAhora va tomando mejor forma, así se ve luego de pre-procesamiento de texto.\n\n\n1.14 Tokenization\n\nnew_vocab_size = 597\nnew_text_vectorizer = TextVectorization(max_tokens=new_vocab_size,\n                                        standardize='lower',\n                                        output_mode='int',\n                                        output_sequence_length=500, # Longitud de salida\n                                        pad_to_max_tokens=500,\n                                        )\n\nnew_text_vectorizer.adapt(texto_lista)\nvectorized_text_fake = new_text_vectorizer(texto_lista)\n\nTokenizamos el texto de la noticia, modifique unos hiperparametros como el maximo de tokens colocandole exacto al texto, del resto los mismos al que use en el modelo.\n\nvectorized_text_fake\n\n&lt;tf.Tensor: shape=(1, 500), dtype=int64, numpy=\narray([[ 52,  71,  34,   5, 268,   7,  22,  25,  15,  26, 101, 410,   6,\n         95, 246,   9,   2,  36, 542, 379,  34, 335,  85, 441,  10,  14,\n        126,   7,   2, 120,   7,  46,  11,  34,  16, 486, 457,   4,   2,\n        562, 490, 489,   7,  52,  71, 409,   6, 101,  95,  34, 150, 206,\n         18,  28, 582,   2, 140,  79, 337,  11,  34, 273, 559, 138,   9,\n        414, 418, 303,   2, 492, 299, 138, 438,  12, 353,   7, 472,   9,\n          8,   1,  17,   5,  11,  21, 485, 501,  26, 131, 355,  12,   5,\n         11,  21, 496, 319, 555,   6,  14, 225,  39,  55,  19,  14,  42,\n         13,  48,  50, 174,  29,   2,  60,  38,   9, 473,   2,  11, 434,\n        221, 363, 103,  36, 338,   8,   3,  31,  22,  25,   4, 153,   8,\n        295,   2, 205, 235, 255,   9,  28,  79, 275, 411,  12, 150, 534,\n        390, 131, 459,   8, 465, 156,  44,   4, 109,  78, 585,  17,  45,\n        364,  45,  30,  32,   5,  11,  21, 468,   9, 197, 172, 251,  18,\n        482, 570,  68,  85, 384, 574,  10,  86, 171,   7,   2,  49, 594,\n        186,  28,  59,   2,  49,  75,  28,  82, 478,   3,  45,  30,   6,\n         32, 196, 302, 596,  18,   5,  68, 494, 112, 349,  70,  15,   8,\n        366,  17,  45,  30,   4,  14, 381,  78,  19,   2, 519, 184,  28,\n         59,   4,   1, 347, 219, 156,  44,   4, 518,  30,  16,   5,  11,\n         21, 467,  73, 592,   3, 567,   5,  11, 426,  63,  58,   4,   1,\n          3, 279,  30,  16, 149,   5,  58,   7,   2, 584, 533,   7, 522,\n          4, 464, 182, 561,  11,   1,   3, 484,   5, 240, 389,   1,  30,\n        177,   1, 575,  10,  57, 500,  17,   2,  46,  11, 236, 483, 406,\n        153,   8,  32,   5, 556, 356,   5, 476,   6,   5,  39, 224, 193,\n        520,  58,   7,   2, 371, 146,  64, 407,  26, 576,  81, 145,  10,\n         33, 393, 211, 220, 183, 230,   3, 283,  41, 248,  81, 452,  29,\n          2, 514,   7,   2, 152, 581,  68, 357,  76,   3,  64,  73, 277,\n        123,   6,  62, 198, 311, 551, 264, 105,  19,   2,  11, 435, 392,\n         64, 123,  47, 321,  19, 481, 146,  53,   2,  11,  21,   3, 159,\n          1,   3,   1,  65, 204, 301,   4,   2, 369,   3, 351, 257,   4,\n        162, 170, 531, 274,   3,   5, 129,  63,   3, 137, 133,   4, 521,\n         65, 203,  11,  21,  96,  93,   9,   2,  21, 297, 167, 560,   4,\n        442,  10,  87, 129,  63,   3, 137, 316, 133,   4, 368,  65, 202,\n         16,  13, 525, 325, 242, 541,   2, 532, 329,   7,  71,  60,  38,\n         24,  27,   6, 573,  10, 161, 526,   5, 281, 401,  98, 124,  53,\n          3,  26, 479, 374, 139,  13,   8,  16,  43, 446,  12,   2,  24,\n         27, 189,   8, 250,  18,  77,  25,   7,  20,  23, 108,  10,   2,\n        148,  10,  24, 310,  15,   8,  35,  43, 288, 151,  14,  55,  19,\n         13,   8,   4,  14, 513, 416,  27,  16,   2,  38,   9, 315,   2,\n        397, 569,   2,  11,  21,   4]])&gt;\n\n\nUna impresión de como se ve la noticia luego de tokenizar, un tensor con valores enteros\n\n\n1.15 Predict a fakenews\n\nprediction = model.predict(vectorized_text_fake)\n\n1/1 [==============================] - 0s 24ms/step\n\n\n\nprediction\n\narray([[0.00081492]], dtype=float32)\n\n\nYa con el texto tokenizado, vectorizado y en una lista, procedemos a aplicar un predict con nuestro modelo, y una impresión de como se ve el valor del array\n\nif prediction &gt;= 0.5:\n    print(\"La noticia es verdadera.\")\nelse:\n    print(\"La noticia es falsa.\")\n\nLa noticia es falsa.\n\n\nCon un if sabremos si la noticia es verdadera o falsa, si su valor es igual o mayor a 0.5 es verdadera, si no se cumple, es falsa\n##Parse a new\n\n\n1.16 Load data\n\n\n1.17 HTML extraction\n\nurl_2 = \"https://www.washingtonpost.com/sports/2022/11/14/world-cup-female-referee-kathryn-nesbitt/\"\n\nDefinimos una variable con la dirección de la pagina web donde proviene la noticia\n\nresponse = requests.get(url_2)\nif response.status_code != 200:\n    raise Exception(\"URL was not found\")\n\ntext_2 = response.text\n\nAcá sacamos toda la información de la pagina web, en caso de ser una dirección erronea, me arrojaria una advertencia\n\n#display(text_2)\n\nLo mismo con la segunda noticia, aca con el texto de la noticia sin pre-procesar\n\n\n1.18 Text Processing\n\nsoup = BeautifulSoup(text_2, \"html.parser\")\n\n\n#main_2 = soup.find(class_='meteredContent grid-center')\n\n\narticle_content = soup.find_all(class_='meteredContent grid-center')\n\narticle_text = []\nfor content in article_content:\n    article_text.append(content.get_text())\n\nfull_article_text = '\\n'.join(article_text)\nfull_article_text = [full_article_text]\n\nAcá definimos una variable con la extracción del texto del HTLM, luego iteramos por parrafos y seleccionado con la clase especificamente la noticia y todo almacenandolo en la variable full_article_text. Para aplicarle la tokenización tuve que hacerla lista, por eso lo de la linea 8.\n\n#print(full_article_text)\n\nImprimimos la segunda noticia luego de pre-procesamiento.\n\n\n1.19 Tokenization\n\nnew_vocab_size_1 = 642\nnew_vectorizer_2 = TextVectorization(max_tokens=new_vocab_size_1,\n                                        standardize='lower',\n                                        output_mode='int',\n                                        output_sequence_length=500, # Longitud de salida\n                                        pad_to_max_tokens=500,\n                                          )\n\nnew_vectorizer_2.adapt(full_article_text)\nvectorized_text_true = new_vectorizer_2(full_article_text)\n\nTokenizamos el texto de la noticia, modifique unos hiperparametros como el maximo de tokens colocandole exacto al texto, del resto los mismos al que use en el modelo.\n\nvectorized_text_true\n\n&lt;tf.Tensor: shape=(1, 500), dtype=int64, numpy=\narray([[454,  10,  43, 279,   3, 575, 640, 381, 155,   4,  80,  45,   6,\n         18,  32, 190,   4, 167,   7, 349,   9, 310, 621,   5, 184,   6,\n        253,   3, 282,  12,   2, 117, 363, 112, 517, 193,  78,  10, 441,\n         12, 514,   5, 303,  14,  30,  35,  20,  13,   2,  86,  17,  74,\n          7, 263, 148,  72,   9,  35, 107, 366,  13,  92,  51,   5, 142,\n         27,  32, 512, 230,  49, 578, 368, 465,  11, 574,   7, 343,   2,\n        376,   8,  86,  18, 105,  11, 255,   6,  43, 619, 133, 225,  25,\n         18,  14, 192,  23, 558,   8,  24,  44, 213,   9, 628, 451,   2,\n          1,  25,  17,  22,  15,  16, 485,   4,   2, 211, 266, 614,   6,\n        418,  10, 426,  53,   3, 374,  11,  15, 579,   4,   9,  77,  27,\n          2, 291,   8,   2, 276, 361, 280,  43,  67, 469,  56, 239, 546,\n        125,  87,  65,  13,  11,  17,  74,  46,  21, 199,   5, 307,  56,\n         21, 583,  58, 244,  40, 407,   1,  82, 340,  13,   2,  31,  11,\n          4, 387,   5,  34,   1,  93,  21,  15, 115,   5, 573,  81,   8,\n        116,  31,   6, 554, 133,  54, 457,  10, 297,   9, 142,   5, 391,\n          9,   1,   1,   4,   1,   7, 602,  24,  35,  20,   8,   2,  60,\n        473,   6,  79,   2,  29,  88,   5,  65,  30,  24,  22, 524,   3,\n        145, 118, 126, 153,  19,   2, 591,  11, 383,  18,   4, 400,   1,\n        609,   1,   6,   2, 610,  19, 251,   9,  23,  25,  17,  22, 106,\n          1, 156,   2,  31, 529, 284, 503, 497, 624,   1,  70, 367,   8,\n        104,  12,   2,   1,  17,  22,   4, 348,   2, 377,  55, 234,   7,\n        119, 489,   3, 294, 121,   1,  10, 186, 208, 309,  37, 219,   6,\n         99, 528, 209,   2, 224, 144, 267,   4,  70,   1,  15, 537,   2,\n         29, 146, 104,   4,   2,   1, 476,   8,   2,  25,  17,  74,  23,\n        287,  87, 162,   2,   1, 393,   4,  27,   2,   1,  35, 103,   7,\n         99,   2, 438, 173, 453, 207,  33, 498, 459, 270,  13, 132, 582,\n         34, 629,  11,  39,  26, 490,  10, 337, 171,  21, 357, 455, 161,\n          2, 317,  12,   2, 403,   1, 414, 358, 135,  14,   3,  20,  41,\n          3, 605,   4, 355,   2, 466,  13,   9, 565,   6,   1,  13,   2,\n        101, 592,  46,   4, 617,   9, 112,   5,   3,  25,  17,  22,   1,\n          7, 295, 482,  19, 156, 515,  85,   4, 541, 608,   1,   2, 479,\n        272,   4, 147,  11,   7, 562,  40, 424, 138,   3,  57, 412, 389,\n          6,  17,  22,  20,  47, 114, 302,  14,   2, 566,   8, 305, 122,\n        395,  13,   2, 215, 354,  20, 386, 172, 561, 299,  12,   1,   7,\n        300, 501,  12, 135,   6,   7, 563, 147,  11,   7, 365,  59,   5,\n          1,  38, 500,  48, 131,   4,   2, 311, 531,  41,  48, 131,  27,\n          2,  18, 530,  18, 110,   4,   9, 179,  10,  26,   3, 323,   1,\n        289,  37,   9, 437, 618,  44,   4, 100, 408,  61,   7,  29, 201,\n          5,  16,  30,  35,  20,   1]])&gt;\n\n\nUna impresión de como se ve la noticia luego de tokenizar, un tensor con valores enteros\n\n\n1.20 Predict a news\n\nprediction = model.predict(vectorized_text_true)\n\n1/1 [==============================] - 0s 39ms/step\n\n\n\nprediction\n\narray([[0.97472566]], dtype=float32)\n\n\nYa con el texto tokenizado, vectorizado y en una lista, procedemos a aplicar un predict con nuestro modelo, y una impresión de como se ve el valor del array\n\nif prediction &gt;= 0.5:\n    print(\"La noticia es verdadera.\")\nelse:\n    print(\"La noticia es falsa.\")\n\nLa noticia es verdadera.\n\n\nAl igual que la primera predicción usaremos un if para saber si la noticia es verdadera o falsa. En este caso es mas cercano a 1, lo que nos informa que la noticia es verdadera.\n\n\n2 Conclusion\nSe nos entrego un dataset con noticias, las cuales se dividian en falsas o verdaderas; Procedimos a analizar la calidad de los datos, los cuales posteriormente procesamos y limpiamos para su función final; Ya con las muetras limpias, entrenamos nuestra red neuronal artificial capaz de predecir una noticia falsa o verdadera, del mundo real; Se llevo acabo numerosas tecnicas para lograr un buen desempeño en el modelo el cual fue bastante preciso en su predicción."
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#data-preprocessing",
    "href": "Projects/Project_01/scripts/analysis.html#data-preprocessing",
    "title": "1 Context",
    "section": "1.3 Data Preprocessing",
    "text": "1.3 Data Preprocessing\n\ndf.drop(columns=['Unnamed: 0'], inplace=True)\n\nAcá borramos dicha columna que no usaremos\n\ndf.dropna(inplace=True)\n\n\ndf = df.reset_index(drop=True)\n\nEliminamos los valores faltantes, y reestablecemos los indices del dataframe"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#univariate-analysis",
    "href": "Projects/Project_01/scripts/analysis.html#univariate-analysis",
    "title": "1 Context",
    "section": "1.4 Univariate Analysis",
    "text": "1.4 Univariate Analysis\n\ndf['label'].value_counts().plot.pie(autopct='%.2f')\n\n\n\n\n\n\n\n\n\ndf['label'].value_counts()\n\n1    36509\n0    35028\nName: label, dtype: int64\n\n\nMediante la grafica pie o pastel, observamos que la columna label que contiene si la notica es falsa o verdadera, esta muy equilibrada bastante parejo"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#train-and-test-data",
    "href": "Projects/Project_01/scripts/analysis.html#train-and-test-data",
    "title": "1 Context",
    "section": "1.5 Train and test data",
    "text": "1.5 Train and test data\n\nX = df['text']\ny = df['label']\n\n# Dividir los datos en conjunto de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=42)\n\nAntes de llevar acabo la tokenización, definimos nuestras variables X y y, en este caso usare solo la columna text para entrenar mi modelo, la columna title no creo que aporte al modelo, al ser un clickbait aportaria datos erroneos al modelo.\n##Tokenization\n\n# Definir el TextVectorization con salida de enteros\ntext_vectorizer = TextVectorization(max_tokens=10000,\n                                    output_mode='int', # El tipo de salida\n                                    standardize='lower', # Optimizador\n                                    output_sequence_length=500, # Longitud de salida\n                                    pad_to_max_tokens=500,\n                                    )\ntext_vectorizer.adapt(X_train)\nX_train = text_vectorizer(X_train)\nX_test = text_vectorizer(X_test)\n\nDefinimos una variable con el TextVectorization para transformar el texto y poder trabajar los datos, le aplicamos unos cuantos hiperparametros para mejorar la salida de los datos.\nAdaptamos con la variable X_train y luego vectorizamos sobreescribiendo las variables de conjunto X"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#define-the-neural-network",
    "href": "Projects/Project_01/scripts/analysis.html#define-the-neural-network",
    "title": "1 Context",
    "section": "1.6 Define the Neural Network",
    "text": "1.6 Define the Neural Network\n\n# Definir el modelo\nmodel = Sequential()\n\nmodel.add(Embedding(10000, 128, input_length=500))\n\n\nmodel.add(LSTM(128))\n\n# Agregar una capa densa final con activación sigmoid para la clasificación binaria\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 500, 128)          1280000   \n                                                                 \n lstm (LSTM)                 (None, 128)               131584    \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1411713 (5.39 MB)\nTrainable params: 1411713 (5.39 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nDefinimos nuestro modelo, en este caso utilizando la arquitectura sequential en Keras, consta de una capa de embedding(128 dim con vocabulario de 10.000) para convertir los números enteros en vectores de embeddings, seguida de una capa LSTM(128Unidades) para modelar las dependencias temporales en los datos de secuencia, y finalmente una capa densa con activación sigmoidal para la clasificación binaria."
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#compile-the-model",
    "href": "Projects/Project_01/scripts/analysis.html#compile-the-model",
    "title": "1 Context",
    "section": "1.7 Compile the model",
    "text": "1.7 Compile the model\n\n# Compilamos el modelo\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nCompilamos el modelo especificando la función de pérdida y el optimizador, este caso con binary_crossentropy y adam, con la metrica de desempeño accuracy"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#train-the-model",
    "href": "Projects/Project_01/scripts/analysis.html#train-the-model",
    "title": "1 Context",
    "section": "1.8 Train the model",
    "text": "1.8 Train the model\n\n# Entrenamos el modelo\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=42)\n\nEpoch 1/3\n1363/1363 [==============================] - 87s 61ms/step - loss: 0.5365 - accuracy: 0.7056\nEpoch 2/3\n1363/1363 [==============================] - 39s 29ms/step - loss: 0.1237 - accuracy: 0.9578\nEpoch 3/3\n1363/1363 [==============================] - 32s 23ms/step - loss: 0.0371 - accuracy: 0.9887\n\n\nEntrenamos nuestro modelo el cual definimos como history, y lo haremos con 3 epocas en este caso. En la sección de evaluación del modelo lo explicare."
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#evaluate-the-model",
    "href": "Projects/Project_01/scripts/analysis.html#evaluate-the-model",
    "title": "1 Context",
    "section": "1.9 Evaluate the model",
    "text": "1.9 Evaluate the model\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax1.plot(history.history['loss'])\nax1.set_title('Función de pérdida del conjunto de entrenamiento')\nax2 = fig.add_subplot(2,1,2, sharex= ax1)\nax2.plot(history.history['accuracy'])\nax2.set_title('Precisión del conjunto de entrenamiento')\n\nplt.setp(ax1.get_xticklabels(), visible=False)\n\nplt.show()\n\n\n\n\n\n\n\n\nGraficamos la función de perdida y la precisión del modelo conforme avanza de epocas en el entrenamiento. Ya en la epoca 2 se consiguio el 95% de precisión con una perdida de información del 12%, a la epoca 3 era suficiente para el entrenamiento cuidando los recursos disponibles\n\n# Evaluar el modelo en el conjunto de prueba\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Loss: {loss}, Accuracy: {accuracy}')\n\n448/448 [==============================] - 4s 8ms/step - loss: 0.0447 - accuracy: 0.9848\nLoss: 0.04466693475842476, Accuracy: 0.9848336577415466\n\n\nEvaluamos nuestro modelo con las metricas de precisión y función de perdida. Nos arroja una perdida de información de un casí 4% y una precisión del 98%"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#predict-test-data",
    "href": "Projects/Project_01/scripts/analysis.html#predict-test-data",
    "title": "1 Context",
    "section": "1.10 Predict test data",
    "text": "1.10 Predict test data\n\npredicciones = model.predict(X_test)\nprint(predicciones)\n\n448/448 [==============================] - 4s 7ms/step\n[[9.9916887e-01]\n [4.8479729e-04]\n [2.8683492e-03]\n ...\n [9.9893659e-01]\n [3.7822025e-03]\n [9.9452496e-01]]\n\n\nAplicamos un predict con nuestro modelo al conjunto de prueba X_test, y lo definimos como predicciones\n\npredicciones_flat = predicciones.flatten()\npredic_si = DataFrame({'Real': y_test,\n                     'Predicción': predicciones_flat})\npredic_si.head(10)\n\n\n  \n    \n\n\n\n\n\n\nReal\nPredicción\n\n\n\n\n53925\n1\n0.999169\n\n\n6939\n0\n0.000485\n\n\n28713\n0\n0.002868\n\n\n65420\n1\n0.996083\n\n\n2286\n0\n0.001370\n\n\n34184\n1\n0.998988\n\n\n29103\n1\n0.994738\n\n\n26688\n0\n0.001819\n\n\n42956\n1\n0.958652\n\n\n65156\n1\n0.999581\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nLes mostrare una comparación con los datos reales frente a los predichos, para poder mostrarlo en un dataframe, aplane las muestras para poder trabajarlas en un dataframe\n\numbral = 0.5  # Umbral de decisión para la clasificación binaria\npredicciones_binarias = np.where(predicciones &gt;= umbral, 1, 0)\n\n# Calcular la matriz de confusión\ncm = confusion_matrix(y_test, predicciones_binarias)\n\nPara lograr graficar nuestra matriz de confusión y observar el desempeño, ambas varibles deben ser de valor 0 o 1, es decir, es un problema de clasificación binaria. Definimos una variable con un umbrar definido, de manera que los “rendondee” a 0 o 1\n\nheatmap(cm, annot= True, fmt= '.0f', cmap= 'winter')\nplt.title('Matriz de confusión')\nplt.xlabel('Valor predicho')\nplt.ylabel('Valor real')\nplt.show()\n\n\n\n\n\n\n\n\nGraficamos el desempeño del modelo con esta matriz de confusión la cual nos arroja un buen desempeño, ya que ha acertado en su gran mayoria a los True positive  y True negative\nYa con nuestro modelo listo, lo usaremos para corroborar si las siguientes noticias son falsas o verdaderas.\n#Parse a fakenew"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#load-data",
    "href": "Projects/Project_01/scripts/analysis.html#load-data",
    "title": "1 Context",
    "section": "1.11 Load Data",
    "text": "1.11 Load Data"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#htlm-extraction",
    "href": "Projects/Project_01/scripts/analysis.html#htlm-extraction",
    "title": "1 Context",
    "section": "1.12 HTLM extraction",
    "text": "1.12 HTLM extraction\n\nurl = \"https://www.breitbart.com/politics/2016/09/10/exposed-fbi-director-james-comeys-clinton-foundation-connection/\"\n\nDefinimos una variable con la dirección de la pagina web donde proviene la noticia\n\nresponse = requests.get(url)\nif response.status_code != 200:\n    raise Exception(\"URL was not found\")\n\ntext = response.text\n\nAcá sacamos toda la información de la pagina web, en caso de ser una dirección erronea, me arrojaria una advertencia\n\n#display(text)\n\nAsí se ve la noticia antes del pre-procesamiento"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#text-processing",
    "href": "Projects/Project_01/scripts/analysis.html#text-processing",
    "title": "1 Context",
    "section": "1.13 Text Processing",
    "text": "1.13 Text Processing\n\nsoup = BeautifulSoup(text, \"html.parser\")\n\n\nmain = soup.find(id=\"MainW\")\n\nparagraphs = main.findAll(\"p\")\n\ncontent = \"\"\nfor paragraph in paragraphs:\n    content = content + paragraph.get_text()\n\ntexto_lista = [content]\n\nAcá definimos una variable con la extracción del texto del HTLM, luego iteramos por parrafos y seleccionado con el id especificamente la noticia y todo almacenandolo en la variable texto lista. Para aplicarle la tokenización tuve que hacerla lista, por eso lo de la linea 9.\n\n#print(texto_lista)\n\nAhora va tomando mejor forma, así se ve luego de pre-procesamiento de texto."
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#tokenization",
    "href": "Projects/Project_01/scripts/analysis.html#tokenization",
    "title": "1 Context",
    "section": "1.14 Tokenization",
    "text": "1.14 Tokenization\n\nnew_vocab_size = 597\nnew_text_vectorizer = TextVectorization(max_tokens=new_vocab_size,\n                                        standardize='lower',\n                                        output_mode='int',\n                                        output_sequence_length=500, # Longitud de salida\n                                        pad_to_max_tokens=500,\n                                        )\n\nnew_text_vectorizer.adapt(texto_lista)\nvectorized_text_fake = new_text_vectorizer(texto_lista)\n\nTokenizamos el texto de la noticia, modifique unos hiperparametros como el maximo de tokens colocandole exacto al texto, del resto los mismos al que use en el modelo.\n\nvectorized_text_fake\n\n&lt;tf.Tensor: shape=(1, 500), dtype=int64, numpy=\narray([[ 52,  71,  34,   5, 268,   7,  22,  25,  15,  26, 101, 410,   6,\n         95, 246,   9,   2,  36, 542, 379,  34, 335,  85, 441,  10,  14,\n        126,   7,   2, 120,   7,  46,  11,  34,  16, 486, 457,   4,   2,\n        562, 490, 489,   7,  52,  71, 409,   6, 101,  95,  34, 150, 206,\n         18,  28, 582,   2, 140,  79, 337,  11,  34, 273, 559, 138,   9,\n        414, 418, 303,   2, 492, 299, 138, 438,  12, 353,   7, 472,   9,\n          8,   1,  17,   5,  11,  21, 485, 501,  26, 131, 355,  12,   5,\n         11,  21, 496, 319, 555,   6,  14, 225,  39,  55,  19,  14,  42,\n         13,  48,  50, 174,  29,   2,  60,  38,   9, 473,   2,  11, 434,\n        221, 363, 103,  36, 338,   8,   3,  31,  22,  25,   4, 153,   8,\n        295,   2, 205, 235, 255,   9,  28,  79, 275, 411,  12, 150, 534,\n        390, 131, 459,   8, 465, 156,  44,   4, 109,  78, 585,  17,  45,\n        364,  45,  30,  32,   5,  11,  21, 468,   9, 197, 172, 251,  18,\n        482, 570,  68,  85, 384, 574,  10,  86, 171,   7,   2,  49, 594,\n        186,  28,  59,   2,  49,  75,  28,  82, 478,   3,  45,  30,   6,\n         32, 196, 302, 596,  18,   5,  68, 494, 112, 349,  70,  15,   8,\n        366,  17,  45,  30,   4,  14, 381,  78,  19,   2, 519, 184,  28,\n         59,   4,   1, 347, 219, 156,  44,   4, 518,  30,  16,   5,  11,\n         21, 467,  73, 592,   3, 567,   5,  11, 426,  63,  58,   4,   1,\n          3, 279,  30,  16, 149,   5,  58,   7,   2, 584, 533,   7, 522,\n          4, 464, 182, 561,  11,   1,   3, 484,   5, 240, 389,   1,  30,\n        177,   1, 575,  10,  57, 500,  17,   2,  46,  11, 236, 483, 406,\n        153,   8,  32,   5, 556, 356,   5, 476,   6,   5,  39, 224, 193,\n        520,  58,   7,   2, 371, 146,  64, 407,  26, 576,  81, 145,  10,\n         33, 393, 211, 220, 183, 230,   3, 283,  41, 248,  81, 452,  29,\n          2, 514,   7,   2, 152, 581,  68, 357,  76,   3,  64,  73, 277,\n        123,   6,  62, 198, 311, 551, 264, 105,  19,   2,  11, 435, 392,\n         64, 123,  47, 321,  19, 481, 146,  53,   2,  11,  21,   3, 159,\n          1,   3,   1,  65, 204, 301,   4,   2, 369,   3, 351, 257,   4,\n        162, 170, 531, 274,   3,   5, 129,  63,   3, 137, 133,   4, 521,\n         65, 203,  11,  21,  96,  93,   9,   2,  21, 297, 167, 560,   4,\n        442,  10,  87, 129,  63,   3, 137, 316, 133,   4, 368,  65, 202,\n         16,  13, 525, 325, 242, 541,   2, 532, 329,   7,  71,  60,  38,\n         24,  27,   6, 573,  10, 161, 526,   5, 281, 401,  98, 124,  53,\n          3,  26, 479, 374, 139,  13,   8,  16,  43, 446,  12,   2,  24,\n         27, 189,   8, 250,  18,  77,  25,   7,  20,  23, 108,  10,   2,\n        148,  10,  24, 310,  15,   8,  35,  43, 288, 151,  14,  55,  19,\n         13,   8,   4,  14, 513, 416,  27,  16,   2,  38,   9, 315,   2,\n        397, 569,   2,  11,  21,   4]])&gt;\n\n\nUna impresión de como se ve la noticia luego de tokenizar, un tensor con valores enteros"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#predict-a-fakenews",
    "href": "Projects/Project_01/scripts/analysis.html#predict-a-fakenews",
    "title": "1 Context",
    "section": "1.15 Predict a fakenews",
    "text": "1.15 Predict a fakenews\n\nprediction = model.predict(vectorized_text_fake)\n\n1/1 [==============================] - 0s 24ms/step\n\n\n\nprediction\n\narray([[0.00081492]], dtype=float32)\n\n\nYa con el texto tokenizado, vectorizado y en una lista, procedemos a aplicar un predict con nuestro modelo, y una impresión de como se ve el valor del array\n\nif prediction &gt;= 0.5:\n    print(\"La noticia es verdadera.\")\nelse:\n    print(\"La noticia es falsa.\")\n\nLa noticia es falsa.\n\n\nCon un if sabremos si la noticia es verdadera o falsa, si su valor es igual o mayor a 0.5 es verdadera, si no se cumple, es falsa\n##Parse a new"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#load-data-1",
    "href": "Projects/Project_01/scripts/analysis.html#load-data-1",
    "title": "1 Context",
    "section": "1.16 Load data",
    "text": "1.16 Load data"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#html-extraction",
    "href": "Projects/Project_01/scripts/analysis.html#html-extraction",
    "title": "1 Context",
    "section": "1.17 HTML extraction",
    "text": "1.17 HTML extraction\n\nurl_2 = \"https://www.washingtonpost.com/sports/2022/11/14/world-cup-female-referee-kathryn-nesbitt/\"\n\nDefinimos una variable con la dirección de la pagina web donde proviene la noticia\n\nresponse = requests.get(url_2)\nif response.status_code != 200:\n    raise Exception(\"URL was not found\")\n\ntext_2 = response.text\n\nAcá sacamos toda la información de la pagina web, en caso de ser una dirección erronea, me arrojaria una advertencia\n\n#display(text_2)\n\nLo mismo con la segunda noticia, aca con el texto de la noticia sin pre-procesar"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#text-processing-1",
    "href": "Projects/Project_01/scripts/analysis.html#text-processing-1",
    "title": "1 Context",
    "section": "1.18 Text Processing",
    "text": "1.18 Text Processing\n\nsoup = BeautifulSoup(text_2, \"html.parser\")\n\n\n#main_2 = soup.find(class_='meteredContent grid-center')\n\n\narticle_content = soup.find_all(class_='meteredContent grid-center')\n\narticle_text = []\nfor content in article_content:\n    article_text.append(content.get_text())\n\nfull_article_text = '\\n'.join(article_text)\nfull_article_text = [full_article_text]\n\nAcá definimos una variable con la extracción del texto del HTLM, luego iteramos por parrafos y seleccionado con la clase especificamente la noticia y todo almacenandolo en la variable full_article_text. Para aplicarle la tokenización tuve que hacerla lista, por eso lo de la linea 8.\n\n#print(full_article_text)\n\nImprimimos la segunda noticia luego de pre-procesamiento."
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#tokenization-1",
    "href": "Projects/Project_01/scripts/analysis.html#tokenization-1",
    "title": "1 Context",
    "section": "1.19 Tokenization",
    "text": "1.19 Tokenization\n\nnew_vocab_size_1 = 642\nnew_vectorizer_2 = TextVectorization(max_tokens=new_vocab_size_1,\n                                        standardize='lower',\n                                        output_mode='int',\n                                        output_sequence_length=500, # Longitud de salida\n                                        pad_to_max_tokens=500,\n                                          )\n\nnew_vectorizer_2.adapt(full_article_text)\nvectorized_text_true = new_vectorizer_2(full_article_text)\n\nTokenizamos el texto de la noticia, modifique unos hiperparametros como el maximo de tokens colocandole exacto al texto, del resto los mismos al que use en el modelo.\n\nvectorized_text_true\n\n&lt;tf.Tensor: shape=(1, 500), dtype=int64, numpy=\narray([[454,  10,  43, 279,   3, 575, 640, 381, 155,   4,  80,  45,   6,\n         18,  32, 190,   4, 167,   7, 349,   9, 310, 621,   5, 184,   6,\n        253,   3, 282,  12,   2, 117, 363, 112, 517, 193,  78,  10, 441,\n         12, 514,   5, 303,  14,  30,  35,  20,  13,   2,  86,  17,  74,\n          7, 263, 148,  72,   9,  35, 107, 366,  13,  92,  51,   5, 142,\n         27,  32, 512, 230,  49, 578, 368, 465,  11, 574,   7, 343,   2,\n        376,   8,  86,  18, 105,  11, 255,   6,  43, 619, 133, 225,  25,\n         18,  14, 192,  23, 558,   8,  24,  44, 213,   9, 628, 451,   2,\n          1,  25,  17,  22,  15,  16, 485,   4,   2, 211, 266, 614,   6,\n        418,  10, 426,  53,   3, 374,  11,  15, 579,   4,   9,  77,  27,\n          2, 291,   8,   2, 276, 361, 280,  43,  67, 469,  56, 239, 546,\n        125,  87,  65,  13,  11,  17,  74,  46,  21, 199,   5, 307,  56,\n         21, 583,  58, 244,  40, 407,   1,  82, 340,  13,   2,  31,  11,\n          4, 387,   5,  34,   1,  93,  21,  15, 115,   5, 573,  81,   8,\n        116,  31,   6, 554, 133,  54, 457,  10, 297,   9, 142,   5, 391,\n          9,   1,   1,   4,   1,   7, 602,  24,  35,  20,   8,   2,  60,\n        473,   6,  79,   2,  29,  88,   5,  65,  30,  24,  22, 524,   3,\n        145, 118, 126, 153,  19,   2, 591,  11, 383,  18,   4, 400,   1,\n        609,   1,   6,   2, 610,  19, 251,   9,  23,  25,  17,  22, 106,\n          1, 156,   2,  31, 529, 284, 503, 497, 624,   1,  70, 367,   8,\n        104,  12,   2,   1,  17,  22,   4, 348,   2, 377,  55, 234,   7,\n        119, 489,   3, 294, 121,   1,  10, 186, 208, 309,  37, 219,   6,\n         99, 528, 209,   2, 224, 144, 267,   4,  70,   1,  15, 537,   2,\n         29, 146, 104,   4,   2,   1, 476,   8,   2,  25,  17,  74,  23,\n        287,  87, 162,   2,   1, 393,   4,  27,   2,   1,  35, 103,   7,\n         99,   2, 438, 173, 453, 207,  33, 498, 459, 270,  13, 132, 582,\n         34, 629,  11,  39,  26, 490,  10, 337, 171,  21, 357, 455, 161,\n          2, 317,  12,   2, 403,   1, 414, 358, 135,  14,   3,  20,  41,\n          3, 605,   4, 355,   2, 466,  13,   9, 565,   6,   1,  13,   2,\n        101, 592,  46,   4, 617,   9, 112,   5,   3,  25,  17,  22,   1,\n          7, 295, 482,  19, 156, 515,  85,   4, 541, 608,   1,   2, 479,\n        272,   4, 147,  11,   7, 562,  40, 424, 138,   3,  57, 412, 389,\n          6,  17,  22,  20,  47, 114, 302,  14,   2, 566,   8, 305, 122,\n        395,  13,   2, 215, 354,  20, 386, 172, 561, 299,  12,   1,   7,\n        300, 501,  12, 135,   6,   7, 563, 147,  11,   7, 365,  59,   5,\n          1,  38, 500,  48, 131,   4,   2, 311, 531,  41,  48, 131,  27,\n          2,  18, 530,  18, 110,   4,   9, 179,  10,  26,   3, 323,   1,\n        289,  37,   9, 437, 618,  44,   4, 100, 408,  61,   7,  29, 201,\n          5,  16,  30,  35,  20,   1]])&gt;\n\n\nUna impresión de como se ve la noticia luego de tokenizar, un tensor con valores enteros"
  },
  {
    "objectID": "Projects/Project_01/scripts/analysis.html#predict-a-news",
    "href": "Projects/Project_01/scripts/analysis.html#predict-a-news",
    "title": "1 Context",
    "section": "1.20 Predict a news",
    "text": "1.20 Predict a news\n\nprediction = model.predict(vectorized_text_true)\n\n1/1 [==============================] - 0s 39ms/step\n\n\n\nprediction\n\narray([[0.97472566]], dtype=float32)\n\n\nYa con el texto tokenizado, vectorizado y en una lista, procedemos a aplicar un predict con nuestro modelo, y una impresión de como se ve el valor del array\n\nif prediction &gt;= 0.5:\n    print(\"La noticia es verdadera.\")\nelse:\n    print(\"La noticia es falsa.\")\n\nLa noticia es verdadera.\n\n\nAl igual que la primera predicción usaremos un if para saber si la noticia es verdadera o falsa. En este caso es mas cercano a 1, lo que nos informa que la noticia es verdadera."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "Soy un científico de datos recién egresado con experiencia en diversos proyectos de análisis de datos y machine learning. A continuación, encontrarás enlaces a algunos de mis trabajos destacados.\n\n\n\n\n\nDescription: Exploratory data analysis using pandas and matplotlib. Data engineering and text processing. Deep learning.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, tensorflow keras.\n\n\n\n\n\nDescription: This project focuses on the exploration and analysis of data using clustering techniques and principal component analysis (PCA). Algorithms such as K-Means, Hierarchical Clustering, and Spectral Clustering are used to identify patterns and groupings in the data.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, scipy, plotly.\n\n\n\n\n\nDescription: This project involves the development of two models: one for predicting the expected salary of job applicants based on their characteristics, and another for classifying whether an applicant is likely to be hired. The project includes data exploration, feature transformation, and model building for both regression and classification tasks.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, Jupyter Notebooks\n\n\n\n\n\nEach project has its own folder and contains a README.md with detailed instructions on how to run the code. Below is a general example of how to clone the repository and install the necessary dependencies:\n\nClone the repository: sh     git clone https://github.com/luismmachados/Portfolio_project\nNavigate to the project directory: sh     cd Portfolio_project\nInstall the dependencies: sh     pip install -r requirements.txt\nRun the project’s main script: sh     python scripts/analysis.py\n\n\n\n\n\nLinkedIn: Luis Manuel Machado\nEmail: luismachado.mmachado@gmail.com\n\n\nThank you for visiting my portfolio! Feel free to contact me if you have any questions or if you would like to discuss potential collaboration opportunities."
  },
  {
    "objectID": "index.html#proyectos",
    "href": "index.html#proyectos",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "Description: Exploratory data analysis using pandas and matplotlib. Data engineering and text processing. Deep learning.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, tensorflow keras.\n\n\n\n\n\nDescription: This project focuses on the exploration and analysis of data using clustering techniques and principal component analysis (PCA). Algorithms such as K-Means, Hierarchical Clustering, and Spectral Clustering are used to identify patterns and groupings in the data.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, scipy, plotly.\n\n\n\n\n\nDescription: This project involves the development of two models: one for predicting the expected salary of job applicants based on their characteristics, and another for classifying whether an applicant is likely to be hired. The project includes data exploration, feature transformation, and model building for both regression and classification tasks.\nTechnologies: Python, pandas, numpy, scikit-learn, matplotlib, seaborn, Jupyter Notebooks"
  },
  {
    "objectID": "index.html#how-to-run-the-projects",
    "href": "index.html#how-to-run-the-projects",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "Each project has its own folder and contains a README.md with detailed instructions on how to run the code. Below is a general example of how to clone the repository and install the necessary dependencies:\n\nClone the repository: sh     git clone https://github.com/luismmachados/Portfolio_project\nNavigate to the project directory: sh     cd Portfolio_project\nInstall the dependencies: sh     pip install -r requirements.txt\nRun the project’s main script: sh     python scripts/analysis.py"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "LinkedIn: Luis Manuel Machado\nEmail: luismachado.mmachado@gmail.com\n\n\nThank you for visiting my portfolio! Feel free to contact me if you have any questions or if you would like to discuss potential collaboration opportunities."
  },
  {
    "objectID": "my-portfolio.html",
    "href": "my-portfolio.html",
    "title": "my-portfolio",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "my-portfolio.html#quarto",
    "href": "my-portfolio.html#quarto",
    "title": "my-portfolio",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#context",
    "href": "Projects/Project_03/Analysis_.html#context",
    "title": "1 Regresor model",
    "section": "0.1 Context",
    "text": "0.1 Context\nUn equipo de Recursos Humanos te acaba de proporcionar el archivo que has descargado. En este se indican algunas características que ellos creen que influyen en el salario que buscan algunas personas que se postulan en empresas de tecnología.\nEste equipo pide de tu apoyo para diseñar un modelo que pueda predecir cuánto pedirá cada postulante con base en las siguientes características:\n\nExperiencia: Años de experiencia en el área.\nPosición: Posición que busca; existen tres opciones:\n\nAnalista\nCoordinador\nGerente\n\nHijos: Número de hijos que tiene.\nCasado: ¿El postulante está casado?\n\n0: No\n1: Sí\n\nEducación: ¿Cuál es el grado máximo de estudios concluido? Existen tres opciones:\n\nBachillerato\nLicenciatura\nPosgrado\n\nSalario: Cantidad en pesos mexicanos que pide. Esta variable es la que se pretende predecir en este ejercicio."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#import-libraries",
    "href": "Projects/Project_03/Analysis_.html#import-libraries",
    "title": "1 Regresor model",
    "section": "1.1 Import Libraries",
    "text": "1.1 Import Libraries\n\nfrom pandas import (read_csv,\n                    DataFrame,\n                    concat,\n                    get_dummies,\n)\n\n\nfrom numpy import (bincount,\n                   sqrt,\n                   logspace,\n                   argmax,\n                   arange,\n)\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom seaborn import (heatmap,\n                    scatterplot,\n                    kdeplot,\n                    displot,\n                    set_style,\n                    lineplot,\n                    boxplot,\n                    pairplot,\n                    catplot,\n)\n\n\nfrom sklearn.linear_model import (LinearRegression,\n                                  Lasso,\n                                  LassoCV,\n                                  RidgeCV,\n                                  Ridge,\n                                  LogisticRegression,\n)\nfrom sklearn.metrics import (accuracy_score,\n                             confusion_matrix,\n                             ConfusionMatrixDisplay,\n                             mean_squared_error,\n                             r2_score,\n                             roc_auc_score,\n                             roc_curve,\n                             classification_report,\n)\nfrom sklearn.preprocessing import (StandardScaler,\n                                   LabelEncoder,\n                                   RobustScaler,\n                                   MinMaxScaler,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\n\n\nimport joblib\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n#Configuración\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#load-data",
    "href": "Projects/Project_03/Analysis_.html#load-data",
    "title": "1 Regresor model",
    "section": "1.2 Load data",
    "text": "1.2 Load data\n\n# Definimos nuestra variable con el Dataframe a trabajar, con el nombre df_1\ndf = read_csv(\"/content/drive/MyDrive/Colab Notebooks/proyectos/rrhh.csv\")\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nExperiencia\nPosicion\nHijos\nCasado\nEducacion\nSalario\n\n\n\n\n0\n0.5\nAnalista\n4\n1\nBachillerato\n13540\n\n\n1\n5.6\nAnalista\n2\n0\nLicenciatura\n31240\n\n\n2\n11.7\nAnalista\n1\n0\nLicenciatura\n63880\n\n\n3\n5.2\nAnalista\n2\n1\nLicenciatura\n29960\n\n\n4\n7.2\nCoordinador\n2\n1\nLicenciatura\n34710\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nYa con nuestro dataset en el drive, podremos cargarlo y definirlo como df, y observamos como se ve."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#eda",
    "href": "Projects/Project_03/Analysis_.html#eda",
    "title": "1 Regresor model",
    "section": "1.3 EDA",
    "text": "1.3 EDA\n\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Experiencia  1000 non-null   float64\n 1   Posicion     1000 non-null   object \n 2   Hijos        1000 non-null   int64  \n 3   Casado       1000 non-null   int64  \n 4   Educacion    1000 non-null   object \n 5   Salario      1000 non-null   int64  \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 47.0+ KB\nNone\n\n\nAcá podemos observar la información del dataset, desde el total de muestras hasta el tipo de dato.\n\ndf.describe().T\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nExperiencia\n1000.0\n6.4162\n2.577612\n0.1\n4.6\n6.4\n8.1\n15.2\n\n\nHijos\n1000.0\n1.4530\n1.208823\n0.0\n0.0\n1.0\n2.0\n4.0\n\n\nCasado\n1000.0\n0.5950\n0.491138\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\nSalario\n1000.0\n30787.0600\n11520.200049\n5630.0\n22557.5\n29470.0\n37922.5\n83470.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAcá observamos las estadisticas descriptivas del dataset, el feature de Salario nos indica su dispersión de manera significativa, reflejando una variabilidad considerable entre los salarios de los empleados. Se puede apreciar entre la diferencia de salarios entre el cuartil 25% y el 75%.\n\ndf.isna().sum()\n\nExperiencia    0\nPosicion       0\nHijos          0\nCasado         0\nEducacion      0\nSalario        0\ndtype: int64\n\n\nEl dataset cuenta con toda su completitud\n\ndf.duplicated().sum()\n\n0\n\n\nNo contiene valores duplicados."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#univariate-analysis",
    "href": "Projects/Project_03/Analysis_.html#univariate-analysis",
    "title": "1 Regresor model",
    "section": "1.4 Univariate analysis",
    "text": "1.4 Univariate analysis\n\ndf.columns\n\nIndex(['Experiencia', 'Posicion', 'Hijos', 'Casado', 'Educacion', 'Salario'], dtype='object')\n\n\n\n1.4.1 Experiencia column\n\ndf['Experiencia'].unique()\n\narray([ 0.5,  5.6, 11.7,  5.2,  7.2,  1.3,  8.1,  6.2,  9.8, 10.5,  6.4,\n        3.7,  8. ,  9. ,  1.7, 12.2,  7. ,  5.9,  1.6,  9.7,  5. ,  6.6,\n        9.6,  7.8, 11.9,  6.3,  4.5,  4.9,  8.4,  8.8,  9.1,  7.5,  5.8,\n        9.3,  5.5,  3.9,  6.9,  2.2,  4.2,  9.9,  5.4, 10.1,  0.9,  9.2,\n        8.2,  8.5,  7.4,  6.1,  7.7,  2.8,  3.2,  5.3,  0.8, 11.5,  3. ,\n        7.1,  2.1,  7.6,  8.6,  6.5, 10.9,  4.8,  4.3, 13. , 10.2,  6.8,\n        5.7,  4.4,  7.9, 10.6,  8.7,  3.1,  8.3,  4.6,  6. , 10.8,  2.4,\n       15.2,  2.6,  1.4,  4.1,  3.3,  2. ,  3.5,  0.6,  6.7,  4.7,  1. ,\n        7.3,  8.9,  9.5,  1.2, 12.9,  5.1,  3.8, 10.4,  3.6, 13.5, 10.7,\n        9.4, 11.3, 11.1,  2.5,  0.7, 11.8,  2.7,  3.4,  1.8, 10. ,  4. ,\n       12.5, 11.4,  0.1, 14.4, 14.3, 11.6,  1.9, 12.4, 12.3, 10.3,  2.9,\n       14.8,  1.1, 11.2, 14.1, 13.7, 12.1])\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.hist(df['Experiencia'], bins=20, edgecolor='black')\nplt.xlabel('Experiencia')\nplt.ylabel('Frecuencia')\nplt.title('Grafica de experiencia')\nplt.show()\n\n\n\n\n\n\n\n\nCuando una gráfica tiene forma de campana, como esta, se le llama distribución normal o distribución gaussiana, entre los 4 - 8.5 se encuentra la mayor concentración de la experiencia de los usuarios\n\n\n1.4.2 Posición column\n\ndf['Posicion'].unique()\n\narray(['Analista', 'Coordinador', 'Gerente'], dtype=object)\n\n\n\nplt.figure(figsize=(6,6))\ndf['Posicion'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Posiciones')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\ndf['Posicion'].value_counts().plot(kind='bar', color=['blue', 'orange', 'green'])\nplt.xlabel('')\nplt.ylabel('Cantidad')\nplt.title('Posiciones')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\nCon estas graficas podemos ver los cargos y lo que representa en su totalidad, en su mayoria son analistas por otra parte la minoria de parte de los gerentes\n\n\n1.4.3 Hijos column\n\ndf['Hijos'].nunique()\n\n5\n\n\n\nplt.figure(figsize=(6,6))\ndf['Hijos'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Hijos')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nCon casi el 60% de los usuarios cuentan con 0 o 1 hijo, luego son cada vez menos las personas que cuentan con mayor cantidad de hijos.\n\n\n1.4.4 Casado column\n\ndf['Casado'].nunique()\n\n2\n\n\n\nplt.figure(figsize=(6,6))\ndf['Casado'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Casado')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nCasi se aprecia un equilibrio entre las personas casadas o los que no lo estan, siendo las personas casadas la mayoria con casi un 60%\n\n\n1.4.5 Educacion column\n\ndf['Educacion'].nunique()\n\n3\n\n\n\nplt.figure(figsize=(6,6))\ndf['Educacion'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\nplt.title('Educación')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nEn su mayoria solo cuentan con la educación de bachillerato, tiene relación com aquellos usuarios que cuentan con el cargo mas bajo de analista, donde tambien un pequeño porcentaje de licenciados son analistas, ya que del 10% de los usuarios con posgrado* solo el 6% son gerentes\n\n\n1.4.6 Salario column\n\ndf['Salario'].describe()\n\ncount     1000.000000\nmean     30787.060000\nstd      11520.200049\nmin       5630.000000\n25%      22557.500000\n50%      29470.000000\n75%      37922.500000\nmax      83470.000000\nName: Salario, dtype: float64\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.hist(df['Salario'], bins=20, edgecolor='black')\nplt.xlabel('Salario')\nplt.ylabel('Frecuencia')\nplt.title('Distribución de salarios')\nplt.show()\n\n\n\n\n\n\n\n\nPodemos observar la forma de una campana en esta grafica, una distribución normal; La mayor concentración de personas se encuentran entre 25.000$ - 35.000$"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#label-encoder",
    "href": "Projects/Project_03/Analysis_.html#label-encoder",
    "title": "1 Regresor model",
    "section": "1.5 Label encoder",
    "text": "1.5 Label encoder\n\nle = LabelEncoder()\n\nfor col in df.columns:                         # Iteramos entre las columnas object y aplicamos entre ellas\n    if df[col].dtype == 'object':\n        le.fit_transform(list(df[col].values))\n        df[col] = le.transform(df[col].values)\n\nPara poder llevar a cabo un analisis bivariado o multivariado los features deben ser numericos, por eso con label encoder, transformaremos aquellos features los cuales cuentan con valores categoricos"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#bivariate-analysis",
    "href": "Projects/Project_03/Analysis_.html#bivariate-analysis",
    "title": "1 Regresor model",
    "section": "1.6 Bivariate Analysis",
    "text": "1.6 Bivariate Analysis\n\ndf.columns\n\nIndex(['Experiencia', 'Posicion', 'Hijos', 'Casado', 'Educacion', 'Salario'], dtype='object')\n\n\n\ncorr = df.corr()\nheatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Mapa de Calor de Correlación')\nplt.show()\n\n\n\n\n\n\n\n\nAntes de los análisis multivariados es necesario obtener la relación de las variables entre si, con este mapa de calor de correlación lo podemos obsevar, por ejemplo siendo Salario el features con mayor correlación entre las variables, usaremos esta gráfica para desarrollar los siguientes análisis\n\nexp_sal  = df.groupby('Experiencia')['Salario'].sum().sort_values(ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nexp_sal.plot(kind='bar', color='salmon')\nplt.xlabel('Experiencia')\nplt.ylabel('Salario')\nplt.title('Salario contra experiencia')\nplt.xticks(rotation=45, ha='right')  # Rotar las etiquetas del eje x para mayor claridad\nplt.show()\n\n\n\n\n\n\n\n\nEsta gráfica nos indica como se comporta el Salario con respecto a la Experiencia, vemos una tendencia la cual mientras más experiencia mas salario, podriamos pensar que la mayor de las experiencias tendrian el mayor de los salarios pero en este caso no es así\n\nplt.figure(figsize=(8, 6))\nscatterplot(data=df, x='Salario', y='Experiencia', marker='o')\n\n# Añadir títulos y etiquetas\nplt.title('Relación entre Salario y experiencia')\nplt.xlabel('Salario')\nplt.ylabel('Experiencia')\nplt.grid(True)\n\n\n\n\n\n\n\n\nCon esta gráfica de dispersión nos deja ver con mas claridad el comportamiento de ambos features, se logra apreciar que los salarios mas altos no corresponde a los usuarios con mayor experiencia, como se dijo anteriormente\n\nLeyenda de posiciones:\n\n0: Analista\n1: Coordinador\n2: Gerente\n\nLeyenda de educacion\n\n0: Bachillerato\n1: Licenciatura\n2: Posgrado\n\n\n\nplt.figure(figsize=(8, 6))\nboxplot(x='Posicion', y='Salario', data=df)\nplt.title('Distribución de Salario por Posición')\nplt.xlabel('Posición')\nplt.ylabel('Salario ($)')\nplt.show()\n\n\n\n\n\n\n\n\nLas gráficas de cajas son muy utiles, podemos ver las Posiciones de los usuarios con respecto al Salario, en la primera posición que seria de analista se observan valores atípicos que representan salarios mas altos de lo “normal”, del resto se comportan de forma “normal”, siendo la posición de gerente con mayor alcance salarial\n\nplt.figure(figsize=(8, 6))\nboxplot(x='Educacion', y='Salario', data=df)\nplt.title('Distribución de Salario por Nivel de Educación')\nplt.xlabel('Educación')\nplt.ylabel('Salario ($)')\nplt.show()\n\n\n\n\n\n\n\n\nAcá observamos mas cantidad de valores atípicos entre los features de Salario y Educación, por parte de la licenciatura se observan los Salarios mas altos, nos indica que hay usuarios los cuales cuentan con la posición con mayor alcance salarial, sin contar con la mayor Educación\n\npairplot(df[['Experiencia', 'Posicion', 'Hijos', 'Educacion', 'Salario']])\nplt.show()\n\n\n\n\n\n\n\n\nPodemos observar una gráfica mas general la cual involucra todos los features, nos devuelve parte de lo que ya he mostrado y otras mas, por ejemplo, como la cantidad de hijos dependiendo del salario, por ejemplo el usuario con 4 hijos cuenta con el mayor Salario seguido de un usuario sin hijos."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#train-test-data",
    "href": "Projects/Project_03/Analysis_.html#train-test-data",
    "title": "1 Regresor model",
    "section": "1.7 Train test data",
    "text": "1.7 Train test data\n\nX = df.drop(columns='Salario')\ny = df['Salario']\nX_train, X_test, y_train, y_test = train_test_split(X,                # Ajustes de hiperparametros\n                                                    y,\n                                                    test_size=0.15,\n                                                    random_state=760\n)\n\nAcá definimos las variables dependientes e independientes, posteriormente definimos los conjuntos de entrenamiento y de prueba para nuestro modelo de predicción"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#data-engineering",
    "href": "Projects/Project_03/Analysis_.html#data-engineering",
    "title": "1 Regresor model",
    "section": "1.8 Data engineering",
    "text": "1.8 Data engineering\n\ncolumn = 'Experiencia', 'Hijos'          # Definimos los features a transformar\n\ncolumn_transfor = ColumnTransformer(\n    transformers=[\n        ('escalar', StandardScaler(), column) # Elección del scaler\n    ],\n    remainder='passthrough'\n)\n\n# Crear el pipeline con el transformer\npipeline = Pipeline(steps=[\n    ('preprocesamiento', column_transfor)\n])\n\n# Aplicar el pipeline al conjunto de entrenamiento y prueba\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)\n\nAntes del entrenamiento es necesario aplicar un pre-procesamiento o ingeneria de datos para el correcto entrenamiento de dicho modelo, aplicaremos un pipeline sencillo, primeramente especificamos los features que procesaremos, luego instanciamos el metodo de escalamiento, y se lo aplicamos a los conjuntos de entrenamiento"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#train-model",
    "href": "Projects/Project_03/Analysis_.html#train-model",
    "title": "1 Regresor model",
    "section": "1.9 Train model",
    "text": "1.9 Train model\n\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nDefinimos nuestro modelo de predicción, en este caso de Regresión lineal, y entrenamos con los conjuntos de entrenamiento"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#prediction-and-performance",
    "href": "Projects/Project_03/Analysis_.html#prediction-and-performance",
    "title": "1 Regresor model",
    "section": "1.10 Prediction and performance",
    "text": "1.10 Prediction and performance\n\ny_pred = model_lr.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Error Cuadrático Medio (MSE): {mse}')\nprint(f'Coeficiente de Determinación (R^2): {r2}')\n\nError Cuadrático Medio (MSE): 31026013.02687542\nCoeficiente de Determinación (R^2): 0.7865906308771698\n\n\nDefinimos una variable de predicción, y le aplicamos un par de metricas de desempeño, el error cuadratico medio y el coeficiente de determinación nos arroja un 78% del r2, un valor cercano a 1 indica una fuerte variabilidad con respecto a la variable dependiente, en pocas palabras se ajusta mejor a los datos, en este caso contamos con un 78% lo que nos indica buen desempeño del modelo.\n\ndf_pred = DataFrame({'Real': y_test,\n                     'Predicción': y_pred})\ndf_pred.head(10)\n\n\n  \n    \n\n\n\n\n\n\nReal\nPredicción\n\n\n\n\n995\n21820\n27051.087501\n\n\n404\n54010\n56677.752709\n\n\n651\n19360\n16087.078140\n\n\n32\n31140\n30370.764973\n\n\n609\n35900\n36380.155317\n\n\n567\n36800\n31557.904712\n\n\n694\n43500\n41003.128612\n\n\n545\n65350\n53607.832321\n\n\n878\n32160\n27543.525786\n\n\n775\n17770\n17111.433285\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAcá creamos un Dataframe para mostrar una comparación de los valores predichos por nuestro modelo contra los valores reales, como podemos ver un pequeño margen de falla."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#save-the-model",
    "href": "Projects/Project_03/Analysis_.html#save-the-model",
    "title": "1 Regresor model",
    "section": "1.11 Save the model",
    "text": "1.11 Save the model\n\n# Guardar el modelo\njoblib.dump(model_lr, 'modelo_regresion_lineal.pkl')\n\n['modelo_regresion_lineal.pkl']\n\n\nYa con nuestro modelo entrenado y probado, podremos guardarlo para luego desplegar"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#load-data-1",
    "href": "Projects/Project_03/Analysis_.html#load-data-1",
    "title": "1 Regresor model",
    "section": "2.1 Load Data",
    "text": "2.1 Load Data\n\ndf_1 = read_csv(\"/content/drive/MyDrive/Colab Notebooks/proyectos/rrhh_2.csv\")\ndf_1.head()\n\n\n  \n    \n\n\n\n\n\n\nExperiencia\nPosicion\nHijos\nCasado\nEducacion\nSalario\nContratado\n\n\n\n\n0\n0.5\nAnalista\n4\n1\nBachillerato\n13540\n1\n\n\n1\n5.6\nAnalista\n2\n0\nLicenciatura\n31240\n0\n\n\n2\n11.7\nAnalista\n1\n0\nLicenciatura\n63880\n0\n\n\n3\n5.2\nAnalista\n2\n1\nLicenciatura\n29960\n1\n\n\n4\n7.2\nCoordinador\n2\n1\nLicenciatura\n34710\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nDefinimos este otro dataset como df_1"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#eda-1",
    "href": "Projects/Project_03/Analysis_.html#eda-1",
    "title": "1 Regresor model",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\ndf_1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Experiencia  1000 non-null   float64\n 1   Posicion     1000 non-null   object \n 2   Hijos        1000 non-null   int64  \n 3   Casado       1000 non-null   int64  \n 4   Educacion    1000 non-null   object \n 5   Salario      1000 non-null   int64  \n 6   Contratado   1000 non-null   int64  \ndtypes: float64(1), int64(4), object(2)\nmemory usage: 54.8+ KB\n\n\nAcá podemos observar el total de las columnas, el tipo de dato de cada una y el total de valores de la misma\n\ndf_1.describe()\n\n\n  \n    \n\n\n\n\n\n\nExperiencia\nHijos\nCasado\nSalario\nContratado\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n6.416200\n1.453000\n0.595000\n30787.060000\n0.518000\n\n\nstd\n2.577612\n1.208823\n0.491138\n11520.200049\n0.499926\n\n\nmin\n0.100000\n0.000000\n0.000000\n5630.000000\n0.000000\n\n\n25%\n4.600000\n0.000000\n0.000000\n22557.500000\n0.000000\n\n\n50%\n6.400000\n1.000000\n1.000000\n29470.000000\n1.000000\n\n\n75%\n8.100000\n2.000000\n1.000000\n37922.500000\n1.000000\n\n\nmax\n15.200000\n4.000000\n1.000000\n83470.000000\n1.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nObservamos las estadisticas descriptivas de este dataset\n\ndf_1.isna().sum() / len(df_1) * 100\n\nExperiencia    0.0\nPosicion       0.0\nHijos          0.0\nCasado         0.0\nEducacion      0.0\nSalario        0.0\nContratado     0.0\ndtype: float64\n\n\nPodemos ver la sumatoria de los valores nulos por columna, en este caso no las hay\n\ndf_1.duplicated().sum()\n\n0\n\n\nTampoco exiten valores duplicados en nuestro Dataset"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#label-encoder-1",
    "href": "Projects/Project_03/Analysis_.html#label-encoder-1",
    "title": "1 Regresor model",
    "section": "2.3 Label encoder",
    "text": "2.3 Label encoder\n\nle = LabelEncoder()\n\nfor col in df_1.columns:\n    if df_1[col].dtype == 'object':\n        le.fit_transform(list(df_1[col].values))\n        df_1[col] = le.transform(df_1[col].values)\n\nAplicaremos la misma tecnica anterior para convertir los features object a numericos, ya que son los mismos"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#bivariate-analysis-1",
    "href": "Projects/Project_03/Analysis_.html#bivariate-analysis-1",
    "title": "1 Regresor model",
    "section": "2.4 Bivariate Analysis",
    "text": "2.4 Bivariate Analysis\n\ncorr = df_1.corr()\nheatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Mapa de Calor de Correlación')\nplt.show()\n\n\n\n\n\n\n\n\nAcá podemos confirmar que el dataframe es el mismo, solo con un feature nuevo llamado Contratado, todas las otras relaciones se mantienen y este nuevo feature tiene un poco con Educacion y Hijos\n\nLeyenda de educacion\n\n0: Bachillerato\n1: Licenciatura\n2: Posgrado\n\n\n\n# Suponiendo que tus variables categóricas son 'Posicion', 'Casado' y 'Educacion'\ncatplot(data=df_1, x='Hijos', hue='Contratado', col='Educacion', kind='count')\nplt.show()\n\n\n\n\n\n\n\n\nEs interesante lo que podemos ver acá: - Mientras mayor educación es menos el desempleo - Los usarios con un solo hijo son los que mas coincidencias tienen - Los usuarios con licenciaturas son los mas equilibrados con respecto a los features analisados\nNo profundizare mas con el análisis entre variables ya que es el mismo que el anterior, ya conocemos las tendencias."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#train-test-data-1",
    "href": "Projects/Project_03/Analysis_.html#train-test-data-1",
    "title": "1 Regresor model",
    "section": "2.5 Train test data",
    "text": "2.5 Train test data\n\nX = df_1.drop(columns='Contratado')\ny = df_1['Contratado']\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.15,\n                                                    random_state=34\n)\n\nAcá definimos las variables dependientes e independientes, posteriormente definimos los conjuntos de entrenamiento y de prueba para nuestro modelo de predicción"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#data-engineering-1",
    "href": "Projects/Project_03/Analysis_.html#data-engineering-1",
    "title": "1 Regresor model",
    "section": "2.6 Data engineering",
    "text": "2.6 Data engineering\n\n\ncolumns_standard = ['Experiencia', 'Hijos']\ncolumns_minmax = ['Salario']\n\n# Crear el column transformer con diferentes scalers\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('standard', StandardScaler(), columns_standard),\n        ('minmax', MinMaxScaler(), columns_minmax)\n    ],\n    remainder='passthrough'\n)\n\n# Crear el pipeline\npipeline = Pipeline(steps=[\n    ('preprocessing', column_transformer)\n])\n\n# Aplicar el pipeline al conjunto de entrenamiento y prueba\nX_train_scaled = pipeline.fit_transform(X_train)\nX_test_scaled = pipeline.transform(X_test)\n\nAplicamos igual un pipeline para el pre-procesamiento, a diferencia del modelo anterior, toca escalar Salario como tiene valores totalmente diferentes los otros features, le aplicaremos un escalamiento que se adapte mejor a sus valores."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#train-model-1",
    "href": "Projects/Project_03/Analysis_.html#train-model-1",
    "title": "1 Regresor model",
    "section": "2.7 Train model",
    "text": "2.7 Train model\n\nmodel_lgr = LogisticRegression(penalty='l2', solver='liblinear')\nmodel_lgr.fit(X_train, y_train)\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(solver='liblinear')\n\n\nPara este modelo de clasificación utilizaremos regresión logistica, modificamos un poco los hiperparametros para mejorar el desempeño."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#prediction-and-performance-1",
    "href": "Projects/Project_03/Analysis_.html#prediction-and-performance-1",
    "title": "1 Regresor model",
    "section": "2.8 Prediction and performance",
    "text": "2.8 Prediction and performance\n\ny_predlgr = model_lgr.predict(X_test)\nLgpre =  accuracy_score(y_test, y_predlgr)\nprint(f'Precisión Score: {Lgpre}')\n\nPrecisión Score: 0.8266666666666667\n\n\nDefinimos la variable predictora y tambien la variable con la matriz de confusión, a diferencia del modelo anterior, usaremos accuaracy_score como metrica de desempeño, arrojandonos un 82% de precisión, bastante bien\n\ncm = confusion_matrix(y_test, y_predlgr)\nheatmap(cm, annot= True, fmt= '.0f', cmap= 'winter')\nplt.title('Matriz de confusión')\nplt.xlabel('Valor predecido')\nplt.ylabel('Valor real')\nplt.show()\n\n\n\n\n\n\n\n\nCon una matriz de confusión podemos observar las fallas y acertadas de los valores predichos con respecto a los reales, notamos que son muy pocos los valores incorrectos"
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#roc-curve",
    "href": "Projects/Project_03/Analysis_.html#roc-curve",
    "title": "1 Regresor model",
    "section": "2.9 ROC curve",
    "text": "2.9 ROC curve\n\n# Obtener probabilidades predichas en el conjunto de prueba\ny_probs = model_lgr.predict_proba(X_test)[:, 1]\n\n# Calcular el AUC-ROC para evaluar el rendimiento del modelo\nroc_auc = roc_auc_score(y_test, y_probs)\nprint(f'AUC-ROC: {roc_auc}')\n\n# Dibujar la curva ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\nplt.plot(fpr, tpr, label=f'AUC-ROC = {roc_auc}')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Curva ROC')\nplt.legend()\nplt.show()\n\n# Encontrar el umbral óptimo\noptimal_threshold = thresholds[argmax(tpr - fpr)]\nprint(f'Umbral óptimo: {optimal_threshold}')\n\n# Aplicar el umbral para convertir las probabilidades en etiquetas de clasificación\ny_pred = (y_probs &gt;= optimal_threshold).astype(int)\n\n# Evaluar el rendimiento del modelo con el umbral óptimo\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f'Exactitud (Accuracy): {accuracy}')\nprint('Matriz de Confusión:')\nprint(conf_matrix)\nprint('Reporte de Clasificación:')\nprint(classification_rep)\n\n\nAUC-ROC: 0.8589673417259625\n\n\n\n\n\n\n\n\n\nUmbral óptimo: 0.5017194374414228\nExactitud (Accuracy): 0.8333333333333334\nMatriz de Confusión:\n[[51 12]\n [13 74]]\nReporte de Clasificación:\n              precision    recall  f1-score   support\n\n           0       0.80      0.81      0.80        63\n           1       0.86      0.85      0.86        87\n\n    accuracy                           0.83       150\n   macro avg       0.83      0.83      0.83       150\nweighted avg       0.83      0.83      0.83       150\n\n\n\nUsaremos como metrica la curva ROC perfecta para problemas de clasificación binaria, nos arroja un 85%, un valor cercano a 1 representa un modelo perfecto, mientras que un valor cercano a 0.5 indica un modelo que no es mejor que una predicción aleatoria."
  },
  {
    "objectID": "Projects/Project_03/Analysis_.html#save-the-model-1",
    "href": "Projects/Project_03/Analysis_.html#save-the-model-1",
    "title": "1 Regresor model",
    "section": "2.10 Save the model",
    "text": "2.10 Save the model\n\n# Guardar el modelo\njoblib.dump(model_lgr, 'modelo_regresion_logistica.pkl')\n\n['modelo_regresion_logistica.pkl']\n\n\nYa con nuestro modelo entrenado y probado, podremos guardarlo para luego desplegar"
  }
]